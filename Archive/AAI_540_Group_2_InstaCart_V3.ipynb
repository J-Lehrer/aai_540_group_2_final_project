{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Imj-SZs4qIOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "uhBrCi3MsbZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "p-MMKTQZsi2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decompressing the \"Archive\" files."
      ],
      "metadata": {
        "id": "7xlQkZa_aeZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNmHwGGeWoVw",
        "outputId": "823b7583-18af-4c7d-bee8-46d298a9301c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/drive/MyDrive/MLOps/Instant_Cart\n"
          ]
        }
      ],
      "source": [
        "# Defining the path to the \"Archives\" folder.\n",
        "archive_path = \"/content/drive/MyDrive/MLOps/archive.zip\"\n",
        "extract_path =\"/content/drive/MyDrive/MLOps/Instant_Cart\"\n",
        "\n",
        "# Extract the archive\n",
        "with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(f\"Files extracted to {extract_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the extracted CSV files"
      ],
      "metadata": {
        "id": "V5_HpEmUbNVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_path =\"/content/drive/MyDrive/MLOps/Instant_Cart\""
      ],
      "metadata": {
        "id": "M2AC3416voQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the paths to the files\n",
        "orders_path = os.path.join(extract_path, 'orders.csv')\n",
        "order_products_prior_path = os.path.join(extract_path, 'order_products__prior.csv')\n",
        "order_products_train_path = os.path.join(extract_path, 'order_products__train.csv')\n",
        "aisles_path = os.path.join(extract_path, 'aisles.csv')\n",
        "departments_path = os.path.join(extract_path, 'departments.csv')\n",
        "products_path = os.path.join(extract_path, 'products.csv')\n",
        "\n",
        "# Loading the CSV files into DataFrames\n",
        "orders = pd.read_csv(orders_path)\n",
        "order_products_prior = pd.read_csv(order_products_prior_path)\n",
        "order_products_train = pd.read_csv(order_products_train_path)\n",
        "aisles = pd.read_csv(aisles_path)\n",
        "departments = pd.read_csv(departments_path)\n",
        "products = pd.read_csv(products_path)\n",
        "\n",
        "print(\"Files successfully load!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEBu7idUbRDg",
        "outputId": "7e001383-23b2-4bc8-f95a-843b98776674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files successfully load!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Dataframes"
      ],
      "metadata": {
        "id": "k1cLAsPVcabS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking DataFrame shape and info\n",
        "print(\"Orders:\")\n",
        "print(orders.shape)\n",
        "print(orders.info())\n",
        "print(\"\")\n",
        "print(\"Order Products Prior:\")\n",
        "print(order_products_prior.shape)\n",
        "print(order_products_prior.info())\n",
        "print(\"\")\n",
        "print(\"Order Products Train:\")\n",
        "print(order_products_train.shape)\n",
        "print(order_products_train.info())\n",
        "print(\"\")\n",
        "print(\"Aisles:\")\n",
        "print(aisles.shape)\n",
        "print(aisles.info())\n",
        "print(\"\")\n",
        "print(\"Departments:\")\n",
        "print(departments.shape)\n",
        "print(departments.info())\n",
        "print(\"\")\n",
        "print(\"Products:\")\n",
        "print(products.shape)\n",
        "print(products.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYtB1rkacfaV",
        "outputId": "b8948f2e-c401-47ab-9f41-0bc5d427f8b5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orders:\n",
            "(3421083, 7)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3421083 entries, 0 to 3421082\n",
            "Data columns (total 7 columns):\n",
            " #   Column                  Dtype  \n",
            "---  ------                  -----  \n",
            " 0   order_id                int64  \n",
            " 1   user_id                 int64  \n",
            " 2   eval_set                object \n",
            " 3   order_number            int64  \n",
            " 4   order_dow               int64  \n",
            " 5   order_hour_of_day       int64  \n",
            " 6   days_since_prior_order  float64\n",
            "dtypes: float64(1), int64(5), object(1)\n",
            "memory usage: 182.7+ MB\n",
            "None\n",
            "\n",
            "Order Products Prior:\n",
            "(32434489, 4)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32434489 entries, 0 to 32434488\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Dtype\n",
            "---  ------             -----\n",
            " 0   order_id           int64\n",
            " 1   product_id         int64\n",
            " 2   add_to_cart_order  int64\n",
            " 3   reordered          int64\n",
            "dtypes: int64(4)\n",
            "memory usage: 989.8 MB\n",
            "None\n",
            "\n",
            "Order Products Train:\n",
            "(1384617, 4)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1384617 entries, 0 to 1384616\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count    Dtype\n",
            "---  ------             --------------    -----\n",
            " 0   order_id           1384617 non-null  int64\n",
            " 1   product_id         1384617 non-null  int64\n",
            " 2   add_to_cart_order  1384617 non-null  int64\n",
            " 3   reordered          1384617 non-null  int64\n",
            "dtypes: int64(4)\n",
            "memory usage: 42.3 MB\n",
            "None\n",
            "\n",
            "Aisles:\n",
            "(134, 2)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 134 entries, 0 to 133\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   aisle_id  134 non-null    int64 \n",
            " 1   aisle     134 non-null    object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 2.2+ KB\n",
            "None\n",
            "\n",
            "Departments:\n",
            "(21, 2)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21 entries, 0 to 20\n",
            "Data columns (total 2 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   department_id  21 non-null     int64 \n",
            " 1   department     21 non-null     object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 468.0+ bytes\n",
            "None\n",
            "\n",
            "Products:\n",
            "(49688, 4)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 49688 entries, 0 to 49687\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   product_id     49688 non-null  int64 \n",
            " 1   product_name   49688 non-null  object\n",
            " 2   aisle_id       49688 non-null  int64 \n",
            " 3   department_id  49688 non-null  int64 \n",
            "dtypes: int64(3), object(1)\n",
            "memory usage: 1.5+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep"
      ],
      "metadata": {
        "id": "UbHZQ1Amqdf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate product orders"
      ],
      "metadata": {
        "id": "WWBZh-HVeyeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, I am combining the dataset: order_products_prior.csv and order_products_train files to compute the total frequency of each product."
      ],
      "metadata": {
        "id": "7MJgNdA9e4du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining prior and train datasets\n",
        "all_order_products = pd.concat([order_products_prior, order_products_train])\n",
        "\n",
        "# Calculating product frequency\n",
        "product_frequency = all_order_products.groupby('product_id').size().reset_index(name='order_count')\n",
        "\n",
        "# Getting the top 10,000 most ordered products\n",
        "top_10k_products = product_frequency.nlargest(10000, 'order_count')\n",
        "print(\"Top 10,000 products identified!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXLGTwBHe2UB",
        "outputId": "c33b243e-1b3d-4041-85f0-473904154590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10,000 products identified!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering orders by Top 10,000 products"
      ],
      "metadata": {
        "id": "9pBSoe8vgBcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, I am filtering the orders to include only those that contain one or more of the top 10,000 products"
      ],
      "metadata": {
        "id": "MjLzbXiRgHTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering orders with top 10k products\n",
        "filtered_orders = all_order_products[all_order_products['product_id'].isin(top_10k_products['product_id'])]\n",
        "\n",
        "# Getting the list of relevant order IDs\n",
        "filtered_order_ids = filtered_orders['order_id'].unique()\n",
        "\n",
        "# Filtering the orders DataFrame\n",
        "filtered_orders_df = orders[orders['order_id'].isin(filtered_order_ids)]\n",
        "print(f\"Filtered orders to include only top 10,000 products. Remaining orders: {len(filtered_orders_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHXlbl_AgOQ6",
        "outputId": "71cf548a-9449-4ce7-e8cc-3ce5fb640660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered orders to include only top 10,000 products. Remaining orders: 3321331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_orders_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljwpuP9Nt_N2",
        "outputId": "6fb7412b-d7fa-4390-d1b2-dcc8f7e0bdb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['order_id', 'user_id', 'eval_set', 'order_number', 'order_dow',\n",
            "       'order_hour_of_day', 'days_since_prior_order'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further limiting orders by Size"
      ],
      "metadata": {
        "id": "j0WrEK3Qg6Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous step, the total number of orders were reduced to: 3,321,331. Next, I am further limiting the size by retaining orders with a minimum of (5) items to reduce the dataset size further."
      ],
      "metadata": {
        "id": "Gzy5dC9Lg_QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of items in each order\n",
        "order_item_count = filtered_orders.groupby('order_id').size().reset_index(name='item_count')\n",
        "\n",
        "# Setting the minimum item threshold (e.g., X = 5)\n",
        "X = 5\n",
        "\n",
        "# Filtering orders with at least X items\n",
        "large_orders = order_item_count[order_item_count['item_count'] >= X]\n",
        "\n",
        "# Getting the list of valid order IDs\n",
        "valid_order_ids = large_orders['order_id']\n",
        "\n",
        "# Filtering the original dataset to keep only the valid order IDs\n",
        "filtered_orders_df = filtered_orders[filtered_orders['order_id'].isin(valid_order_ids)]\n",
        "\n",
        "# Counting unique orders in the final filtered dataset\n",
        "unique_orders_count = filtered_orders_df['order_id'].nunique()\n",
        "\n",
        "print(f\"Filtered down to orders with at least {X} items.\")\n",
        "print(f\"Final number of unique orders: {unique_orders_count}\")\n",
        "print(f\"Final number of rows (products): {len(filtered_orders_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOYESoK0oXfR",
        "outputId": "42ee8df4-8f71-4b9b-aa5c-1d6283b9dbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered down to orders with at least 5 items.\n",
            "Final number of unique orders: 2400986\n",
            "Final number of rows (products): 28368235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further limiting order to \"Active Users\""
      ],
      "metadata": {
        "id": "6f5-XkvKiwFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, I limited the orders to include only those that contain only 5 products from the top 10k products. We ended with a total size of: 2,400,986. For further reducing the dataset, I am going to focus on users with consistent purchasing behavior. For example, users with more than \"Y\" total orders. For accomplishing this step, I am going to count the total number of orders per user and filter users with at least 10 orders."
      ],
      "metadata": {
        "id": "OxFMPMs3i80Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_orders_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6utRJKZtqAz",
        "outputId": "d81a2413-399d-4ded-e5d2-78c542ed28fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['order_id', 'product_id', 'add_to_cart_order', 'reordered'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging user_id into filtered_orders_df\n",
        "filtered_orders_df = pd.merge(\n",
        "    filtered_orders_df,\n",
        "    orders[['order_id', 'user_id']],\n",
        "    on='order_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Confirming the user_id column is now included\n",
        "print(filtered_orders_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPfnToP9ughg",
        "outputId": "b327315a-d7d0-41fb-820b-8eb48b9e14a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['order_id', 'product_id', 'add_to_cart_order', 'reordered', 'user_id'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the total number of orders per user\n",
        "user_order_counts = filtered_orders_df.groupby('user_id').size().reset_index(name='order_count')\n",
        "\n",
        "# Setting the threshold for active users (e.g., Y = 10 orders)\n",
        "Y = 10\n",
        "active_users = user_order_counts[user_order_counts['order_count'] >= Y]\n",
        "active_user_ids = active_users['user_id']\n",
        "\n",
        "print(f\"Number of active users with at least {Y} orders: {len(active_user_ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZN37tF4jaD7",
        "outputId": "e20cb112-b179-45ee-cd27-5123b8d1734a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of active users with at least 10 orders: 184340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_orders_active_users = filtered_orders_df[filtered_orders_df['user_id'].isin(active_user_ids)]\n",
        "\n",
        "print(f\"Remaining orders: {len(filtered_orders_active_users)}\")\n",
        "print(f\"Unique users: {filtered_orders_active_users['user_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkabOFp9pwa8",
        "outputId": "8fe5a667-4a47-4e0a-970f-d06ccba6b08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining orders: 28313728\n",
            "Unique users: 184340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unique orders remaining: {filtered_orders_active_users['order_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Awir8r4rLnh",
        "outputId": "f2a31e76-6065-4b9c-81ea-047994038695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique orders remaining: 2391917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_orders_active_users.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z0vyp2Iu6b6",
        "outputId": "f9d6fca1-74b4-42ad-e559-486c2f700ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['order_id', 'product_id', 'add_to_cart_order', 'reordered', 'user_id'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving filtered_orders_active_users DataFram into csv\n",
        "filtered_orders_active_users.to_csv('/content/drive/MyDrive/MLOps/Instant_Cart/filtered_orders_active_users.csv', index=False)"
      ],
      "metadata": {
        "id": "c5F_XLsLu_V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further filtering by top 10 aisles or departments."
      ],
      "metadata": {
        "id": "1lUMMyf-qBIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the previous process, we obtained a total of 2,391,917 orders. This time, I am focusing on the most frequently ordered items within the top 10 departments. I will be identifying the top 10 departments with the highest number of orders for later retaining only the orders and products that belong to these top aisles or departments."
      ],
      "metadata": {
        "id": "sc6C635kq_z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_orders_active_users = pd.read_csv('/content/drive/MyDrive/MLOps/Instant_Cart/filtered_orders_active_users.csv')"
      ],
      "metadata": {
        "id": "up4PiuqDvxOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, I am merging aisles.csv and departments.csv with products.csv to enrich the product information with aisle and department details."
      ],
      "metadata": {
        "id": "0YnOCk7sym09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging products with aisles and departments\n",
        "products_enriched = pd.merge(\n",
        "    products,\n",
        "    aisles,\n",
        "    on='aisle_id',\n",
        "    how='left'\n",
        ")\n",
        "products_enriched = pd.merge(\n",
        "    products_enriched,\n",
        "    departments,\n",
        "    on='department_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Verifying the columns in the enriched product dataset\n",
        "print(products_enriched.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju-tjz1auv94",
        "outputId": "c748ffed-334b-4179-da6c-87373a111b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['product_id', 'product_name', 'aisle_id', 'department_id', 'aisle',\n",
            "       'department'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, I joing the enriched products information (products_enriched) with the (filtered_orders_active_users) DataFrame."
      ],
      "metadata": {
        "id": "gVi1xpfwLGF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging product details into the filtered orders dataset\n",
        "filtered_orders_with_details = pd.merge(\n",
        "    filtered_orders_active_users,\n",
        "    products_enriched,\n",
        "    on='product_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Verifying the merged dataset\n",
        "print(filtered_orders_with_details.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSDE1Tmmv7a8",
        "outputId": "9c066621-8f53-4abf-9cd1-ed11a9f59734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['order_id', 'product_id', 'add_to_cart_order', 'reordered', 'user_id',\n",
            "       'product_name', 'aisle_id', 'department_id', 'aisle', 'department'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I group the data by aisle and department (separate) and calculate the total number of orders. Then, sorting the results in descending order and retain the top 10."
      ],
      "metadata": {
        "id": "YmSFA836LRaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting orders by aisle\n",
        "aisle_order_counts = filtered_orders_with_details.groupby('aisle').size().reset_index(name='order_count')\n",
        "\n",
        "# Getting the top 10 aisles\n",
        "top_aisles = aisle_order_counts.nlargest(10, 'order_count')\n",
        "top_aisle_names = top_aisles['aisle']\n",
        "\n",
        "print(f\"Top 10 aisles: {list(top_aisle_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV6QwZuvwaNH",
        "outputId": "3a9440f7-e9ff-4582-ae26-993f601f4e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 aisles: ['fresh fruits', 'fresh vegetables', 'packaged vegetables fruits', 'yogurt', 'packaged cheese', 'milk', 'water seltzer sparkling water', 'chips pretzels', 'soy lactosefree', 'bread']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting orders by department\n",
        "department_order_counts = filtered_orders_with_details.groupby('department').size().reset_index(name='order_count')\n",
        "\n",
        "# Getting the top 10 departments\n",
        "top_departments = department_order_counts.nlargest(10, 'order_count')\n",
        "top_department_names = top_departments['department']\n",
        "\n",
        "print(f\"Top 10 departments: {list(top_department_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdIUz83AweLD",
        "outputId": "b1430fdb-427f-424d-dd26-d25aed96bf79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 departments: ['produce', 'dairy eggs', 'snacks', 'beverages', 'frozen', 'pantry', 'bakery', 'deli', 'canned goods', 'dry goods pasta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering orders for top aisles\n",
        "filtered_by_aisles = filtered_orders_with_details[filtered_orders_with_details['aisle'].isin(top_aisle_names)]\n",
        "\n",
        "print(f\"Remaining orders after filtering by top aisles: {len(filtered_by_aisles)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IPEfXobwhOm",
        "outputId": "3d8aab3e-6d8a-4047-b918-c19ebf708bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining orders after filtering by top aisles: 14096649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering orders for top departments\n",
        "filtered_by_departments = filtered_orders_with_details[filtered_orders_with_details['department'].isin(top_department_names)]\n",
        "\n",
        "print(f\"Remaining orders after filtering by top departments: {len(filtered_by_departments)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F35nwG4Gwmp7",
        "outputId": "0e2c3a4f-67e6-4927-8948-5615322c5309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining orders after filtering by top departments: 25702383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying aisles or departments in the filtered dataset\n",
        "print(filtered_by_aisles['aisle'].value_counts())\n",
        "print(filtered_by_departments['department'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRNxqET6wsP4",
        "outputId": "1d42f96a-e2cf-4acf-e404-6516ef1d81b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aisle\n",
            "fresh fruits                     3485989\n",
            "fresh vegetables                 3378122\n",
            "packaged vegetables fruits       1686448\n",
            "yogurt                           1364214\n",
            "packaged cheese                   905963\n",
            "milk                              806036\n",
            "water seltzer sparkling water     720034\n",
            "chips pretzels                    635021\n",
            "soy lactosefree                   587908\n",
            "bread                             526914\n",
            "Name: count, dtype: int64\n",
            "department\n",
            "produce            9133122\n",
            "dairy eggs         4988600\n",
            "snacks             2366457\n",
            "beverages          2168493\n",
            "frozen             1888991\n",
            "pantry             1502823\n",
            "bakery             1042682\n",
            "deli                940087\n",
            "canned goods        928885\n",
            "dry goods pasta     742243\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to filter the data by \"Top 10 Departments\" for the following reasons:\n",
        "\n",
        "- It aligns with the project's goal of creatinga resuable ML pipeline, as department-level insights generalize better.\n",
        "- Provides a broader perspective, covering diverse products and trends.\n",
        "- Reduces the dataset size efficiently while retaining valuable data for high-level analysis.\n",
        "\n",
        "Next, I filter the dataset by the top 10 departments: produce, dairy eggs, snacks, etc. Then, I save the filtered dataset into .csv format."
      ],
      "metadata": {
        "id": "KRnHPZ9XL3SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the files\n",
        "output_path = \"/content/drive/MyDrive/MLOps/Instant_Cart/filtered_by_top_departments.csv\"  # Desired output path\n",
        "\n",
        "# Top 10 departments\n",
        "top_departments = [\n",
        "    \"produce\", \"dairy eggs\", \"snacks\", \"beverages\", \"frozen\",\n",
        "    \"pantry\", \"bakery\", \"deli\", \"canned goods\", \"dry goods pasta\"\n",
        "]\n",
        "\n",
        "# Filtering the dataset to include only top departments\n",
        "filtered_by_departments = filtered_orders_with_details[\n",
        "    filtered_orders_with_details['department'].isin(top_departments)\n",
        "]\n",
        "\n",
        "# Saving the filtered dataset\n",
        "filtered_by_departments.to_csv(output_path, index=False)\n",
        "\n",
        "# Summary of the filtered dataset\n",
        "filtered_summary = {\n",
        "    \"Remaining Rows\": len(filtered_by_departments),\n",
        "    \"Unique Orders\": filtered_by_departments['order_id'].nunique(),\n",
        "    \"Unique Users\": filtered_by_departments['user_id'].nunique(),\n",
        "    \"Unique Departments\": filtered_by_departments['department'].nunique()\n",
        "}\n",
        "\n",
        "print(\"Filtered dataset saved successfully.\")\n",
        "print(filtered_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC7lRwTvx5mc",
        "outputId": "f6e4f958-7493-49b0-e14a-727b90bd38b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset saved successfully.\n",
            "{'Remaining Rows': 25702383, 'Unique Orders': 2389985, 'Unique Users': 184304, 'Unique Departments': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further filtering rarely reordered products\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "judCA0QHKyVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the previous filtering process, we obtained a total number of unique orders of: 2,389,985. Now, I will proceed to calculate the reorder rate for each product by grouping the data by product_id and taking the mean of the \"reordered\" column."
      ],
      "metadata": {
        "id": "-HqT1ZsfMkca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the filtered dataset\n",
        "filtered_by_top_departments_path = \"/content/drive/MyDrive/MLOps/Instant_Cart/filtered_by_top_departments.csv\"\n",
        "filtered_by_top_departments = pd.read_csv(filtered_by_top_departments_path)\n",
        "\n",
        "# Calculating reorder rate for each product\n",
        "product_reorder_rate = filtered_by_top_departments.groupby('product_id')['reordered'].mean().reset_index()\n",
        "product_reorder_rate.rename(columns={'reordered': 'reorder_rate'}, inplace=True)\n",
        "\n",
        "# Setting a threshold for rarely reordered products (e.g., 0.2 or 20%)\n",
        "threshold = 0.2\n",
        "frequently_reordered_products = product_reorder_rate[product_reorder_rate['reorder_rate'] > threshold]\n",
        "\n",
        "print(f\"Products with a reorder rate above {threshold}: {len(frequently_reordered_products)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH0Lz4UdMj_B",
        "outputId": "9fa7487f-8f29-4d3d-aeb9-a8be16da398e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products with a reorder rate above 0.2: 8002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering the dataset to include only frequently reordered products\n",
        "filtered_frequent_reorders = filtered_by_top_departments[\n",
        "    filtered_by_top_departments['product_id'].isin(frequently_reordered_products['product_id'])\n",
        "]\n",
        "\n",
        "print(f\"Filtered dataset to include frequently reordered products.\")\n",
        "print(f\"Remaining Rows: {len(filtered_frequent_reorders)}\")\n",
        "print(f\"Unique Orders: {filtered_frequent_reorders['order_id'].nunique()}\")\n",
        "print(f\"Unique Products: {filtered_frequent_reorders['product_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0P6_HJ6NiWc",
        "outputId": "3b4b5056-9b51-451c-9846-17c478139c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset to include frequently reordered products.\n",
            "Remaining Rows: 25454061\n",
            "Unique Orders: 2389931\n",
            "Unique Products: 8002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the filtered dataset\n",
        "output_path_frequent_reorders = \"/content/drive/MyDrive/MLOps/Instant_Cart/filtered_frequent_reorders.csv\"\n",
        "filtered_frequent_reorders.to_csv(output_path_frequent_reorders, index=False)\n",
        "\n",
        "print(f\"Filtered dataset saved at {output_path_frequent_reorders}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS7OxgJaN58g",
        "outputId": "731d7065-2994-4390-adf8-a608a8afde0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset saved at /content/drive/MyDrive/MLOps/Instant_Cart/filtered_frequent_reorders.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering for frequent buyer"
      ],
      "metadata": {
        "id": "shv4TREbPvH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the previous result, the dataset was only reduced to: 2,389,931 orders. Next, I am filtering by identifying frequent buyey. For example: retain users who order top products at least N times."
      ],
      "metadata": {
        "id": "eJGqQmQvPzSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N = 450.\n",
        "This first dataset will be balanced between most frequent buyers and non active users.\n",
        "\n",
        "**Note to consider:**\n",
        "\n",
        "As we increase \"N\", the focus is only on the most active users, which may bias the model toward frequent buyes and their behavior.\n",
        "\n",
        "The model might become less generalize to users with lower purchasing activity.\n",
        "\n",
        "If the dataset is heavily skewed toward a small group of active users, some product-specific might be lost. However, keeping more frequent buyers might still capture sufficient trends highly reordered products.\n",
        "\n",
        "A piperline trained on a subset of frequent buyers may perform for similr groups but may not generalize to less frequent users or broader audience."
      ],
      "metadata": {
        "id": "ogIXoLb6UnGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the threshold for frequent buyers (e.g., at least N = 10 orders)\n",
        "N = 450\n",
        "\n",
        "# Counting the number of orders per user\n",
        "user_order_counts = filtered_frequent_reorders.groupby('user_id').size().reset_index(name='order_count')\n",
        "\n",
        "# Filtering users with at least N orders\n",
        "frequent_buyers = user_order_counts[user_order_counts['order_count'] >= N]\n",
        "frequent_buyer_ids = frequent_buyers['user_id']\n",
        "\n",
        "print(f\"Number of frequent buyers with at least {N} orders: {len(frequent_buyer_ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7emFXikfQFIU",
        "outputId": "481672f8-86e4-4308-bc11-3f4acdf7abeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of frequent buyers with at least 450 orders: 11033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering orders for frequent buyers\n",
        "filtered_frequent_buyers = filtered_frequent_reorders[\n",
        "    filtered_frequent_reorders['user_id'].isin(frequent_buyer_ids)\n",
        "]\n",
        "\n",
        "print(f\"Filtered dataset to include orders from frequent buyers.\")\n",
        "print(f\"Remaining Rows: {len(filtered_frequent_buyers)}\")\n",
        "print(f\"Unique Orders: {filtered_frequent_buyers['order_id'].nunique()}\")\n",
        "print(f\"Unique Users: {filtered_frequent_buyers['user_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvkBeMymQNak",
        "outputId": "fe7a7179-9959-46d7-b1f2-2cb5d62ddc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset to include orders from frequent buyers.\n",
            "Remaining Rows: 7588145\n",
            "Unique Orders: 562019\n",
            "Unique Users: 11033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the filtered dataset\n",
        "output_path_frequent_buyers = \"/content/drive/MyDrive/MLOps/Instant_Cart/filtered_frequent_buyers_v1.csv\"\n",
        "filtered_frequent_buyers.to_csv(output_path_frequent_buyers, index=False)\n",
        "\n",
        "print(f\"Filtered dataset saved at {output_path_frequent_buyers}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrqeNICQQTrt",
        "outputId": "2314f3b5-235f-47ba-ca07-b30d91e2ebd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset saved at /content/drive/MyDrive/MLOps/Instant_Cart/filtered_frequent_buyers_v1.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N = 850"
      ],
      "metadata": {
        "id": "TnlSYio0U6fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset will be for the less generalize ML model."
      ],
      "metadata": {
        "id": "ti4LMkDaV6Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 850\n",
        "\n",
        "# Counting the number of orders per user\n",
        "user_order_counts = filtered_frequent_reorders.groupby('user_id').size().reset_index(name='order_count')\n",
        "\n",
        "# Filtering users with at least N orders\n",
        "frequent_buyers = user_order_counts[user_order_counts['order_count'] >= N]\n",
        "frequent_buyer_ids = frequent_buyers['user_id']\n",
        "\n",
        "print(f\"Number of frequent buyers with at least {N} orders: {len(frequent_buyer_ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QsKNz1NUjnF",
        "outputId": "d7ba335e-0879-4975-e6b8-d48a38721f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of frequent buyers with at least 850 orders: 2081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering orders for frequent buyers\n",
        "filtered_frequent_buyers = filtered_frequent_reorders[\n",
        "    filtered_frequent_reorders['user_id'].isin(frequent_buyer_ids)\n",
        "]\n",
        "\n",
        "print(f\"Filtered dataset to include orders from frequent buyers.\")\n",
        "print(f\"Remaining Rows: {len(filtered_frequent_buyers)}\")\n",
        "print(f\"Unique Orders: {filtered_frequent_buyers['order_id'].nunique()}\")\n",
        "print(f\"Unique Users: {filtered_frequent_buyers['user_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSRcXreDWBuU",
        "outputId": "dcf6eb9b-6245-4500-a8c8-fecd035181b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset to include orders from frequent buyers.\n",
            "Remaining Rows: 2253205\n",
            "Unique Orders: 137382\n",
            "Unique Users: 2081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the filtered dataset\n",
        "output_path_frequent_buyers = \"/content/drive/MyDrive/MLOps/Instant_Cart/filtered_frequent_buyers_v2.csv\"\n",
        "filtered_frequent_buyers.to_csv(output_path_frequent_buyers, index=False)\n",
        "\n",
        "print(f\"Filtered dataset saved at {output_path_frequent_buyers}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8DSbzEvWLs9",
        "outputId": "31ae97d3-7b7e-48b0-f754-bf70dcffceea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset saved at /content/drive/MyDrive/MLOps/Instant_Cart/filtered_frequent_buyers_v2.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "gIU6KSSpqvvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "S-aqSNTsrvT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing necessary libraries"
      ],
      "metadata": {
        "id": "h1A2pvinq3Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyathena"
      ],
      "metadata": {
        "id": "wg6KMTJYqwdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install awswrangler"
      ],
      "metadata": {
        "id": "pE8Hlbdrq9iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "zm1jZi0wq-7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyathena import connect"
      ],
      "metadata": {
        "id": "g00zA_qkrA04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "s1QDDOYIrEtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "j1OrqsWUrGTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting InstantCart CSV dataset into Parquet"
      ],
      "metadata": {
        "id": "3AeuRQ2HrKPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"s3://sagemaker-us-east-1-209611057751/data-lake/project/filtered_frequent_buyers_v1.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "# Loading CSV from S3\n",
        "\n",
        "train_df, remaining_df = train_test_split(df, train_size=0.4, random_state=42)\n",
        "\n",
        "# Split the remaining data into production and temp datasets (66% production, 33% temp)\n",
        "production_df, temp_df = train_test_split(remaining_df, train_size=0.666666, random_state=42)\n",
        "\n",
        "# Split the temp data into test and validation datasets (50% test, 50% validation)\n",
        "test_df, validation_df = train_test_split(temp_df, train_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "oo0-1jHjrNuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining S3 paths\n",
        "parquet_output_path = \"s3://sagemaker-us-east-1-209611057751/data-lake/project/partitioned/\"\n",
        "\n",
        "# Save each split dataset to Parquet with partitioning by 'department'\n",
        "wr.s3.to_parquet(\n",
        "    df=train_df,\n",
        "    path=parquet_output_path + \"train/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "wr.s3.to_parquet(\n",
        "    df=production_df,\n",
        "    path=parquet_output_path + \"production/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "wr.s3.to_parquet(\n",
        "    df=test_df,\n",
        "    path=parquet_output_path + \"test/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "wr.s3.to_parquet(\n",
        "    df=validation_df,\n",
        "    path=parquet_output_path + \"validation/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")"
      ],
      "metadata": {
        "id": "xz9bSD-zrQXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the database if it doesn't exist\n",
        "database_name = \"instacart_db\"\n",
        "try:\n",
        "    # Create the database in AWS Glue\n",
        "    wr.catalog.create_database(name=database_name)\n",
        "    print(f\"Database '{database_name}' created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating database: {e}\")"
      ],
      "metadata": {
        "id": "mwdv_AQLrVm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the Parquet tables in AWS Glue\n",
        "table_name = \"instacart_orders\"\n",
        "\n",
        "wr.catalog.create_parquet_table(\n",
        "    database=database_name,\n",
        "    table=table_name,\n",
        "    path=parquet_output_path + \"train/\",\n",
        "    columns_types={\n",
        "        \"order_id\": \"bigint\",\n",
        "        \"product_id\": \"bigint\",\n",
        "        \"add_to_cart_order\": \"int\",\n",
        "        \"reordered\": \"int\",\n",
        "        \"user_id\": \"bigint\",\n",
        "        \"product_name\": \"string\",\n",
        "        \"aisle_id\": \"int\",\n",
        "        \"department_id\": \"int\",\n",
        "        \"aisle\": \"string\",\n",
        "        \"department\": \"string\"\n",
        "    },\n",
        "    partitions_types={\"department\": \"string\"},\n",
        "    description=\"Partitioned Instacart orders dataset for optimized Athena queries.\"\n",
        ")\n",
        "\n",
        "print(\"Partitioned Parquet table registered in AWS Glue successfully.\")"
      ],
      "metadata": {
        "id": "G-_o0s-HrXkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Database for InstantCart"
      ],
      "metadata": {
        "id": "NmD2q73mraQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyathena import connect\n",
        "\n",
        "# Defineing AWS Resources\n",
        "bucket_name = \"sagemaker-us-east-1-209611057751\"\n",
        "region = \"us-east-1\"\n",
        "database_name = \"instacart_db\"\n",
        "table_name = \"instacart_orders\"\n",
        "s3_data_location = f\"s3://{bucket_name}/data-lake/project/partitioned/train/\"  # Using partitioned dataset\n",
        "\n",
        "# Defining Athena Staging Directory\n",
        "s3_staging_dir = f\"s3://{bucket_name}/athena/instacart_staging/\"\n",
        "\n",
        "# Creating Athena Connection\n",
        "try:\n",
        "    conn = connect(s3_staging_dir=s3_staging_dir, region_name=region)\n",
        "    cursor = conn.cursor()\n",
        "    print(\"Connected to Athena successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Error connecting to Athena:\", e)\n",
        "\n",
        "# Creating Database\n",
        "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
        "cursor.execute(create_db_query)\n",
        "print(f\"Database '{database_name}' created successfully!\")\n",
        "\n",
        "# Verifying Database Creation\n",
        "cursor.execute(\"SHOW DATABASES\")\n",
        "databases = [row[0] for row in cursor.fetchall()]\n",
        "if database_name in databases:\n",
        "    print(f\"Database '{database_name}' exists!\")"
      ],
      "metadata": {
        "id": "j3vTWvY7rcMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Athena database"
      ],
      "metadata": {
        "id": "XA5bXf3krgKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_table_query = f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_name} (\n",
        "    order_id BIGINT,\n",
        "    product_id BIGINT,\n",
        "    add_to_cart_order INT,\n",
        "    reordered INT,\n",
        "    user_id BIGINT,\n",
        "    product_name STRING,\n",
        "    aisle_id INT,\n",
        "    department_id INT,\n",
        "    aisle STRING\n",
        ")\n",
        "PARTITIONED BY (department STRING)  -- Partitioned by 'department'\n",
        "STORED AS PARQUET\n",
        "LOCATION 's3://sagemaker-us-east-1-209611057751/data-lake/project/partitioned/train/'\n",
        "TBLPROPERTIES ('parquet.compression'='SNAPPY');\n",
        "\"\"\"\n",
        "\n",
        "# Execute Table Creation Query\n",
        "cursor.execute(create_table_query)\n",
        "print(f\"Table '{table_name}' created successfully in database '{database_name}'.\")"
      ],
      "metadata": {
        "id": "DzR3CNRMrh4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running MSCK REPAIR to Load Partitions\n",
        "cursor.execute(f\"MSCK REPAIR TABLE {database_name}.{table_name}\")\n",
        "print(\"Partitions updated successfully.\")"
      ],
      "metadata": {
        "id": "lE8TA4P8rkSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(\"SHOW DATABASES\")\n",
        "databases = [row[0] for row in cursor.fetchall()]\n",
        "if database_name in databases:\n",
        "    print(f\"Database '{database_name}' exists in Athena!\")\n",
        "else:\n",
        "    print(f\"Database '{database_name}' does not exist.\")"
      ],
      "metadata": {
        "id": "dPB3DFstrmF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a Sample Query to Verify Data\n",
        "test_query = f\"SELECT count(*) FROM {database_name}.{table_name} ;\"\n",
        "cursor.execute(test_query)\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "print(\"Sample Query Results:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "gcewROSOronT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(f\"SHOW PARTITIONS {database_name}.{table_name}\")\n",
        "partitions = cursor.fetchall()\n",
        "if partitions:\n",
        "    print(f\"Partitions found in table '{table_name}': {partitions}\")\n",
        "else:\n",
        "    print(f\"No partitions found in table '{table_name}'.\")"
      ],
      "metadata": {
        "id": "zfFg6kVPrrkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the Total Orders and Unique Users"
      ],
      "metadata": {
        "id": "wW5EG1sDr7Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First Query: Total Orders & Unique Users\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    COUNT(DISTINCT order_id) AS total_orders,\n",
        "    COUNT(DISTINCT user_id) AS unique_users\n",
        "FROM {database_name}.{table_name};\n",
        "\"\"\"\n",
        "\n",
        "# Executing query\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "# Printing results\n",
        "print(\"Total Orders & Unique Users:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "8ZcbAdOdr8F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 10 Most Ordered Products"
      ],
      "metadata": {
        "id": "67ArKnz6sC_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT product_name, COUNT(*) AS total_orders\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY product_name\n",
        "ORDER BY total_orders DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Top 10 Most Ordered Products:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "6va4eGiWsEuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "#  Bar Chart: Top 10 Most Ordered Products\n",
        "# ===========================\n",
        "\n",
        "products = [\"Banana\", \"Bag of Organic Bananas\", \"Organic Strawberries\", \"Organic Hass Avocado\",\n",
        "            \"Organic Baby Spinach\", \"Organic Raspberries\", \"Organic Avocado\", \"Organic Whole Milk\",\n",
        "            \"Limes\", \"Large Lemon\"]\n",
        "total_orders = [52073, 46964, 39311, 31555, 29459, 21479, 19582, 19211, 17136, 16719]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_products = pd.DataFrame({\"Product Name\": products, \"Total Orders\": total_orders})\n",
        "\n",
        "# Plotting Bar Chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"Total Orders\", y=\"Product Name\", data=df_products, palette=\"viridis\")\n",
        "plt.xlabel(\"Total Orders\")\n",
        "plt.ylabel(\"Product Name\")\n",
        "plt.title(\"Top 10 Most Ordered Products\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pEXLa2ILsHsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights"
      ],
      "metadata": {
        "id": "YRbd3XiisRZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall it can be seen that perishable products are the most ordered products and generally follow a trend of the quicker the product spoils the more often it is ordered. This makes sense as the customers most likely are making smaller orders of those products so as to not let them spoil by having them sit on their counters or refrigerators for an extended period of time. A condensed list of some notable findings can be seen in the list below:\n",
        "\n",
        "*   Bananas are the most ordered item with 52073 orders.\n",
        "*   Organic produce is extremely popular making up 7 out of 10 most ordered items.\n",
        "*   Fruits and vegitables make up 9 out of the top 10 most ordered items.\n",
        "*   All of the top 10 items spoil within a matter of weeks after opening."
      ],
      "metadata": {
        "id": "5zZVSafdsK9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reorder Rate per Product"
      ],
      "metadata": {
        "id": "a8xGtdDGu36R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "    product_name,\n",
        "    COUNT(*) AS total_orders,\n",
        "    SUM(reordered) AS total_reorders,\n",
        "    ROUND(100.0 * SUM(reordered) / COUNT(*), 2) AS reorder_rate\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY product_name\n",
        "ORDER BY reorder_rate DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Top 10 Products with Highest Reorder Rate:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "E34znjAsu8P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights for Top 10 Products with Highest Reorder Rate:"
      ],
      "metadata": {
        "id": "xNVec_EOu_JQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is notable that all of the most reordered items have a 100% reorder rate, indicating that customers who order those items have a strong preference for them specifically. Additional insights as they relate to each product can be seen below:\n",
        "*   **Thirst Quencher Caffeine-Free Naturally Flavored Citrus Soda** and **Smoked Whitefish Salad** have the highest reorder frequency, highlighting that these items are consistently chosen by customers who keep coming back for more.\n",
        "*   **100% Lactose-Free Milk** has a **100% reorder rate**, indicating it is a highly demanded product among lactose-intolerant customers who consistently reorder it.\n",
        "*   **Seltzer Water** and **Sparkling Water, Bottles** as well as **Premium Lots of Pulp Orange Juice** are all drink product meaning that customers who order these product may have a strong preference when it comes to the types of drinks and the brands that they consume."
      ],
      "metadata": {
        "id": "ptFI2WqVvDmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orders by Department"
      ],
      "metadata": {
        "id": "zTfTbUANzimy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT department, COUNT(*) AS total_orders\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY department\n",
        "ORDER BY total_orders DESC;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Orders by Department:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "TuQhzl6HzlLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Bar Chart: Orders by Department\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Department, Total Orders)\n",
        "departments = [\"produce\", \"dairy eggs\", \"snacks\", \"beverages\", \"frozen\",\n",
        "               \"pantry\", \"bakery\", \"deli\", \"canned goods\", \"dry goods pasta\"]\n",
        "total_orders = [1116850, 619999, 294213, 234613, 201359, 138924, 127884, 114349, 102416, 84651]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_orders_by_department = pd.DataFrame({\"Department\": departments, \"Total Orders\": total_orders})\n",
        "\n",
        "# Plotting Bar Chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"Total Orders\", y=\"Department\", data=df_orders_by_department, palette=\"magma\")\n",
        "plt.xlabel(\"Total Orders\")\n",
        "plt.ylabel(\"Department\")\n",
        "plt.title(\"Total Orders by Department\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VF5uPeq5zpUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights"
      ],
      "metadata": {
        "id": "2EitMZJKzsfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsurprisingly, the departments with the most orders generally follow a trend of items that spoil the fastest while adding in a factor of items that are consumed at the highest frequency. Produce and dairy/eggs are atop the list by a sizable margin, followed by items that are consumed on a mostly daily basis, snacks and beverages. Next, frozen items are consumed regularly, but have a longer shelf life so they do not need to be ordered as often. The rest of the top ten departments are almost all non/less perishable items. The exemption to this is deli, which falls into the previously discussed category of items that are consumed regularly and have a shorter shelf life so they are most likely ordered in smaller batches. Additional insights can be seen below in a listed format:\n",
        "*   Produce is the most ordered department with ~1.11 million orders. This aligns with the earlier Top Ordered Products (bananas, avocados, berries).\n",
        "*   Dairy & Eggs ranks second with 619K orders.\n",
        "*   Snacks are the third most popular category (294K orders).\n",
        "*   Beverages (~234K orders) likely include popular items like bottled water, juices, and coffee some of which were in the most reordered top 10.\n",
        "*   Frozen Foods (~201K orders) suggest customers stock up on frozen essentials."
      ],
      "metadata": {
        "id": "YO_5C9nQzw0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Popular Aisles"
      ],
      "metadata": {
        "id": "WXCB8vmA2XRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT aisle, COUNT(*) AS total_orders\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY aisle\n",
        "ORDER BY total_orders DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Top 10 Aisles:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "l47ZosCB2cAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Bar Chart: Most Popular Aisles\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Aisle, Total Orders)\n",
        "aisles = [\"fresh fruits\", \"fresh vegetables\", \"packaged vegetables fruits\", \"yogurt\",\n",
        "          \"packaged cheese\", \"milk\", \"water seltzer sparkling water\", \"chips pretzels\",\n",
        "          \"soy lactosefree\", \"bread\"]\n",
        "aisle_orders = [450026, 404252, 205492, 184903, 112690, 103953, 78019, 74536, 68786, 65469]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_aisles = pd.DataFrame({\"Aisle\": aisles, \"Total Orders\": aisle_orders})\n",
        "\n",
        "# Plotting Bar Chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"Total Orders\", y=\"Aisle\", data=df_aisles, palette=\"coolwarm\")\n",
        "plt.xlabel(\"Total Orders\")\n",
        "plt.ylabel(\"Aisle\")\n",
        "plt.title(\"Top 10 Most Popular Aisles\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O01tVKgc2edB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights"
      ],
      "metadata": {
        "id": "5gIej48U2hYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most popular aisles by total orders reflects much of the same findings that have been shown thus far in the data analysis: those items that are the most perishable are ordered the most often. Aisles that have fresh fruits and vegetables rank the highest by a noticeable amount, followed by packaged fruits and vegetables. Then are the dairy products and a few items that fall into the category of snack foods. Other insights can be seen below:\n",
        "\n",
        "*   Fresh Produce Dominance: Fresh Fruits (450K orders) and Fresh Vegetables (404K orders) are the top two aisles.\n",
        "*   Combined, these two alone account for over 950K orders, which reinforces that Produce is the top department.\n",
        "*   Dairy is Highly Popular: Yogurt (184K orders) and Packaged Cheese (112K orders) show strong demand with the addition of Milk (103K orders) confirms that dairy products are household essentials and ordered regularly.\n",
        "* Beverages have high order numbers: Water, Seltzer, and Sparkling Water (78K orders) ranks #7, showing strong demand for bottled drinks.\n",
        "*   Snacks & Bread are Key Pantry Items: Chips & Pretzels (74K orders) are among the most frequently purchased snacks. While Bread (65K orders) is typically a staple food.\n",
        "*   Plant-Based Alternatives: Soy & Lactose-Free Products (68K orders) indicate consumers with dietary restrictions order items that comply with their diet often."
      ],
      "metadata": {
        "id": "K3M0S_1J2kWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reorder Ratio by Department"
      ],
      "metadata": {
        "id": "UZvpnYpK5Udd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "    department,\n",
        "    COUNT(*) AS total_orders,\n",
        "    SUM(reordered) AS total_reorders,\n",
        "    ROUND(100.0 * SUM(reordered) / COUNT(*), 2) AS reorder_ratio\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY department\n",
        "ORDER BY reorder_ratio DESC;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Reorder Ratio by Department:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "U7RctP8C5Ysf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights"
      ],
      "metadata": {
        "id": "GopYRy205qRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much of the information from the reorder rate by department reflects the trends in the other categories explored so far. The small exception is that beverages and dairy/eggs rank higher on the list than produce. This may be caused by specific preferences in the type of dairy products and beverages as opposed to produce, which may have a higher variance order to order from the same customer. The other departments show bread, deli and snack products are reordered often which aligns with the information that has been gathered with regards to the trend of items that are consumed frequently. Finally, items that have a longer shelf life are reordered less often, possibly indicating that there may be more bulk ordering of these items and there could be orders where these items are not ordered or skipped. Other specifics about the reorder ratio by department can be seen below:\n",
        "\n",
        "*   Dairy & Eggs Have the Highest Reorder Rate (82.73%)\n",
        "*   Beverages Rank #2 in Reorders (80.85%)\n",
        "*   Produce Has a High Reorder Rate (80.71%)\n",
        "*   Bakery (80.43%) & Deli (78.53%) Show Strong Reorder Loyalty\n",
        "*   Snacks & Frozen Foods Have Moderate Reorder Rates (~70%)\n",
        "*   Canned Goods & Dry Goods Have Lower Reorder Rates (~65%)\n",
        "*   Pantry Has the Lowest Reorder Rate (57.26%)"
      ],
      "metadata": {
        "id": "qs0NP5Pw5s0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Pie Chart: Reorder Ratio by Department\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Department, Reorder Ratio)\n",
        "departments = [\"dairy eggs\", \"beverages\", \"produce\", \"bakery\", \"deli\",\n",
        "               \"snacks\", \"frozen\", \"canned goods\", \"dry goods pasta\", \"pantry\"]\n",
        "reorder_ratio = [82.73, 80.85, 80.71, 80.43, 78.53, 74.01, 71.87, 65.51, 65.33, 57.26]\n",
        "\n",
        "# Create DataFrame\n",
        "df_departments = pd.DataFrame({\"Department\": departments, \"Reorder Ratio\": reorder_ratio})\n",
        "\n",
        "# Plot Pie Chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(df_departments[\"Reorder Ratio\"], labels=df_departments[\"Department\"],\n",
        "        autopct=\"%1.1f%%\", colors=sns.color_palette(\"viridis\", len(departments)), startangle=140)\n",
        "plt.title(\"Reorder Ratio by Department\")\n",
        "plt.axis(\"equal\")  # Equal aspect ratio ensures the pie is drawn as a circle\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "naBBcABT8oMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Stacked Bar Chart: Reordered vs Non-Reordered Orders by Department\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Department, Reordered Orders)\n",
        "departments = [\"dairy eggs\", \"beverages\", \"produce\", \"bakery\", \"deli\",\n",
        "               \"snacks\", \"frozen\", \"canned goods\", \"dry goods pasta\", \"pantry\"]\n",
        "total_orders = [619999, 234613, 1116850, 127884, 114349, 294213, 201359, 102416, 84651, 138924]\n",
        "reordered = [512956, 189692, 901358, 102853, 89802, 217758, 144719, 67094, 55299, 79541]\n",
        "non_reordered = [total - reorder for total, reorder in zip(total_orders, reordered)]\n",
        "\n",
        "# Create DataFrame\n",
        "df_reorders = pd.DataFrame({\"Department\": departments, \"Reordered\": reordered, \"Non-Reordered\": non_reordered})\n",
        "\n",
        "# Plot Stacked Bar Chart (Reordered vs Non-Reordered)\n",
        "df_reorders.set_index(\"Department\")[[\"Reordered\", \"Non-Reordered\"]].plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"viridis\")\n",
        "plt.xlabel(\"Department\")\n",
        "plt.ylabel(\"Number of Orders\")\n",
        "plt.title(\"Reordered vs Non-Reordered Orders by Department\")\n",
        "plt.legend([\"Reordered\", \"Non-Reordered\"])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HrZqse9A8tWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis for reordered vs non-reordered orders by department:"
      ],
      "metadata": {
        "id": "i4gzNJc69hLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the items reordered or not by department shows the same trends that have been seen throughout the entire expository data analysis. Produce and dairy/eggs have the highest reorder rate with the snacks, beverages and frozen goods departments all ranking similarly to each other. Additionally pantry, canned goods and dry pasta goods reinforce the trend of a reordering of frequently consumed non-perishable goods."
      ],
      "metadata": {
        "id": "NpKi7eB-8v-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Analysis"
      ],
      "metadata": {
        "id": "lqDgolaG805j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data: Numerical Features for Correlation Analysis\n",
        "departments = [\"dairy eggs\", \"beverages\", \"produce\", \"bakery\", \"deli\",\n",
        "               \"snacks\", \"frozen\", \"canned goods\", \"dry goods pasta\", \"pantry\"]\n",
        "total_orders = [619999, 234613, 1116850, 127884, 114349, 294213, 201359, 102416, 84651, 138924]\n",
        "reordered = [512956, 189692, 901358, 102853, 89802, 217758, 144719, 67094, 55299, 79541]\n",
        "reorder_ratio = [82.73, 80.85, 80.71, 80.43, 78.53, 74.01, 71.87, 65.51, 65.33, 57.26]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_correlation = pd.DataFrame({\n",
        "    \"Total Orders\": total_orders,\n",
        "    \"Reordered Orders\": reordered,\n",
        "    \"Reorder Ratio (%)\": reorder_ratio\n",
        "})\n",
        "\n",
        "# Computing Correlation Matrix\n",
        "correlation_matrix = df_correlation.corr()\n",
        "\n",
        "# Plotting Correlation Heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix of Order Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_uFjPYe082-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The correlation matix need to be redone with departments added"
      ],
      "metadata": {
        "id": "laJNy2PH-i7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA summary"
      ],
      "metadata": {
        "id": "QTzt3RGW_WcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Reorder Ratio: strongly correlated with reorders.\n",
        "2. Total Orders: high correlation with reorder likelihood.\n",
        "3. User Reorder percerntage: Helps preduct if a user is likely to reorder.\n",
        "4. Product Popularity: popular items have higher reorders.\n",
        "5. Department and Aisle reorder ratios: certain categories drive higher reorders"
      ],
      "metadata": {
        "id": "9cdJS318_beD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "sTUsIXWH-rlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For creating ML Feature features, the following labels will be made:\n",
        "*   User ID: This tracks individual purchase behavior.\n",
        "*   Product ID: Helps identify frequently reordered products.\n",
        "*   Department ID: Some departments have higher reorder rates.\n",
        "*   Aisle ID: Aisle-level trends impact reorder likelihood.\n",
        "*   Total Orders: Highly correlated with reorder behavior.\n",
        "*   Reorder Ratio: Strong predictor of repeat purchases.\n",
        "*   Total items in Orders: Determines if larger orders influence reorders.\n",
        "*   User Order Frequency: Identifies frequent vs. occasional buyers.\n",
        "*   User Reorder Percentage: Determines likelihood of repeat purchases.\n",
        "*   Product popularity: Captures demand for the product.\n",
        "*   Department reorder ratio: Some departments have stronger reorder trends.\n",
        "*   Aisle Reorder ratio: Aisle-specific reorder behavior.\n",
        "*   Product Reorder trend: Helps detect seasonal or trending products."
      ],
      "metadata": {
        "id": "JCalS_b0-x3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "IGxl1u0leNGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing necessary libraries"
      ],
      "metadata": {
        "id": "faTCtYcne8MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyathena"
      ],
      "metadata": {
        "id": "3UME8-YdeW5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install awswrangler"
      ],
      "metadata": {
        "id": "OKju5RT8e_if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "E3KB0VtDfB2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade s3fs aiobotocore botocore"
      ],
      "metadata": {
        "id": "X9Ja9ftffDqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import s3fs\n",
        "import botocore\n",
        "\n",
        "print(f\"s3fs version: {s3fs.__version__}\")\n",
        "print(f\"botocore version: {botocore.__version__}\")"
      ],
      "metadata": {
        "id": "zXnPBXkOfFWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "ox9d__vKfITR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyathena import connect"
      ],
      "metadata": {
        "id": "AUGGf6frfLQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_olvZ9LsfNCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "W0t2lRzIfOro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting InstantCart CSV dataset into Parquet"
      ],
      "metadata": {
        "id": "HfdXoUEUfRdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/filtered_frequent_buyers_v1.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "# Loading CSV from S3\n",
        "\n",
        "train_df, remaining_df = train_test_split(df, train_size=0.4, random_state=42)\n",
        "\n",
        "# Split the remaining data into production and temp datasets (66% production, 33% temp)\n",
        "production_df, temp_df = train_test_split(remaining_df, train_size=0.666666, random_state=42)\n",
        "\n",
        "# Split the temp data into test and validation datasets (50% test, 50% validation)\n",
        "test_df, validation_df = train_test_split(temp_df, train_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "iWZKfELBfUDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining S3 paths\n",
        "parquet_output_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/\"\n",
        "\n",
        "\n",
        "# Save each split dataset to Parquet with partitioning by 'department'\n",
        "wr.s3.to_parquet(\n",
        "    df=train_df,\n",
        "    path=parquet_output_path + \"train/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "wr.s3.to_parquet(\n",
        "    df=production_df,\n",
        "    path=parquet_output_path + \"production/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "wr.s3.to_parquet(\n",
        "    df=test_df,\n",
        "    path=parquet_output_path + \"test/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "wr.s3.to_parquet(\n",
        "    df=validation_df,\n",
        "    path=parquet_output_path + \"validation/\",\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")"
      ],
      "metadata": {
        "id": "GTrbgrtlfWGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the database if it doesn't exist\n",
        "database_name = \"instacart_db_split\"\n",
        "try:\n",
        "    # Create the database in AWS Glue\n",
        "    wr.catalog.create_database(name=database_name)\n",
        "    print(f\"Database '{database_name}' created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating database: {e}\")"
      ],
      "metadata": {
        "id": "SB8jAO91fZqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the Parquet tables in AWS Glue\n",
        "table_name = \"instacart_orders\"\n",
        "\n",
        "wr.catalog.create_parquet_table(\n",
        "    database=database_name,\n",
        "    table=table_name,\n",
        "    path=parquet_output_path + \"train/\",\n",
        "    columns_types={\n",
        "        \"order_id\": \"bigint\",\n",
        "        \"product_id\": \"bigint\",\n",
        "        \"add_to_cart_order\": \"int\",\n",
        "        \"reordered\": \"int\",\n",
        "        \"user_id\": \"bigint\",\n",
        "        \"product_name\": \"string\",\n",
        "        \"aisle_id\": \"int\",\n",
        "        \"department_id\": \"int\",\n",
        "        \"aisle\": \"string\",\n",
        "        \"department\": \"string\"\n",
        "    },\n",
        "    partitions_types={\"department\": \"string\"},\n",
        "    description=\"Partitioned Instacart orders dataset for optimized Athena queries.\"\n",
        ")\n",
        "\n",
        "print(\"Partitioned Parquet table registered in AWS Glue successfully.\")"
      ],
      "metadata": {
        "id": "fwAaT9DEfbl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Database for InstantCart"
      ],
      "metadata": {
        "id": "TewRvufhfehh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyathena import connect\n",
        "\n",
        "# Defineing AWS Resources\n",
        "bucket_name = \"sagemaker-us-east-1-921916832724\"\n",
        "region = \"us-east-1\"\n",
        "database_name = \"instacart_db_split\"\n",
        "table_name = \"instacart_orders\"\n",
        "s3_data_location = f\"s3://{bucket_name}/data-lake/Project/partitioned_split/train/\"  # Using partitioned dataset\n",
        "\n",
        "# Defining Athena Staging Directory\n",
        "s3_staging_dir = f\"s3://{bucket_name}/athena/instacart_staging_split/\"\n",
        "\n",
        "# Creating Athena Connection\n",
        "try:\n",
        "    conn = connect(s3_staging_dir=s3_staging_dir, region_name=region)\n",
        "    cursor = conn.cursor()\n",
        "    print(\"Connected to Athena successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Error connecting to Athena:\", e)\n",
        "\n",
        "# Creating Database\n",
        "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
        "cursor.execute(create_db_query)\n",
        "print(f\"Database '{database_name}' created successfully!\")\n",
        "\n",
        "# Verifying Database Creation\n",
        "cursor.execute(\"SHOW DATABASES\")\n",
        "databases = [row[0] for row in cursor.fetchall()]\n",
        "if database_name in databases:\n",
        "    print(f\"Database '{database_name}' exists!\")"
      ],
      "metadata": {
        "id": "8NBQjIMZfdkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Athena database"
      ],
      "metadata": {
        "id": "kpoiii4ofkOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SQL query to create the table\n",
        "create_table_query = \"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS instacart_db_split.instacart_orders (\n",
        "    order_id BIGINT,\n",
        "    product_id BIGINT,\n",
        "    add_to_cart_order INT,\n",
        "    reordered INT,\n",
        "    user_id BIGINT,\n",
        "    product_name STRING,\n",
        "    aisle_id INT,\n",
        "    department_id INT,\n",
        "    aisle STRING\n",
        ")\n",
        "PARTITIONED BY (department STRING)  -- Partitioned by department\n",
        "STORED AS PARQUET\n",
        "LOCATION 's3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/train/'\n",
        "TBLPROPERTIES ('parquet.compression'='SNAPPY');\n",
        "\"\"\"\n",
        "\n",
        "# Execute the SQL query in Athena\n",
        "cursor.execute(create_table_query)\n",
        "print(\"Table 'instacart_orders' created successfully in database 'instacart_db_split'.\")"
      ],
      "metadata": {
        "id": "tqUp-aQEfmJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running MSCK REPAIR to Load Partitions\n",
        "cursor.execute(f\"MSCK REPAIR TABLE {database_name}.{table_name}\")\n",
        "print(\"Partitions updated successfully.\")"
      ],
      "metadata": {
        "id": "78S00WHdfoQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(\"SHOW DATABASES\")\n",
        "databases = [row[0] for row in cursor.fetchall()]\n",
        "if database_name in databases:\n",
        "    print(f\"Database '{database_name}' exists in Athena!\")\n",
        "else:\n",
        "    print(f\"Database '{database_name}' does not exist.\")"
      ],
      "metadata": {
        "id": "IXBkKYBOfqE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "response = s3.list_objects_v2(Bucket='sagemaker-us-east-1-921916832724', Prefix='data-lake/Project/partitioned_split/train/')\n",
        "for obj in response.get('Contents', []):\n",
        "    print(obj['Key'])"
      ],
      "metadata": {
        "id": "ZQbtwvDGfs0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a Sample Query to Verify Data\n",
        "test_query = f\"SELECT count(*) FROM {database_name}.{table_name} ;\"\n",
        "cursor.execute(test_query)\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "print(\"Sample Query Results:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "3mBdTiJffusG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(f\"SHOW PARTITIONS {database_name}.{table_name}\")\n",
        "partitions = cursor.fetchall()\n",
        "if partitions:\n",
        "    print(f\"Partitions found in table '{table_name}': {partitions}\")\n",
        "else:\n",
        "    print(f\"No partitions found in table '{table_name}'.\")"
      ],
      "metadata": {
        "id": "Mhim1e15fxDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis in the training set"
      ],
      "metadata": {
        "id": "76CcKo9Hfyh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the Total Orders and Unique Users"
      ],
      "metadata": {
        "id": "lqfpjLU8f1Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First Query: Total Orders & Unique Users\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    COUNT(DISTINCT order_id) AS total_orders,\n",
        "    COUNT(DISTINCT user_id) AS unique_users\n",
        "FROM {database_name}.{table_name};\n",
        "\"\"\"\n",
        "\n",
        "# Executing query\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "# Printing results\n",
        "print(\"Total Orders & Unique Users:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "0ar1mzXMf0zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 10 Most Ordered Products"
      ],
      "metadata": {
        "id": "dRFlHsmdf6lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT product_name, COUNT(*) AS total_orders\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY product_name\n",
        "ORDER BY total_orders DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Top 10 Most Ordered Products:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "5UQOB5a7f9Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "#  Bar Chart: Top 10 Most Ordered Products\n",
        "# ===========================\n",
        "\n",
        "products = [\"Banana\", \"Bag of Organic Bananas\", \"Organic Strawberries\", \"Organic Hass Avocado\",\n",
        "            \"Organic Baby Spinach\", \"Organic Raspberries\", \"Organic Avocado\", \"Organic Whole Milk\",\n",
        "            \"Limes\", \"Large Lemon\"]\n",
        "total_orders = [52073, 46964, 39311, 31555, 29459, 21479, 19582, 19211, 17136, 16719]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_products = pd.DataFrame({\"Product Name\": products, \"Total Orders\": total_orders})\n",
        "\n",
        "# Plotting Bar Chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"Total Orders\", y=\"Product Name\", data=df_products, palette=\"viridis\")\n",
        "plt.xlabel(\"Total Orders\")\n",
        "plt.ylabel(\"Product Name\")\n",
        "plt.title(\"Top 10 Most Ordered Products\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TujPVk7ZgALr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights\n",
        "\n",
        "- Bananas are the most ordered item with 52073 orders.\n",
        "- Organic produce is extremely popular. Where 7 out of 10 items are organic.\n",
        "- Dairy products like Whole Milk rank in the Top 10.\n",
        "- Cirtrus fruits like Limes and Lemons are in high demand."
      ],
      "metadata": {
        "id": "LqrP4GGfgDBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reorder Rate per Product"
      ],
      "metadata": {
        "id": "k65HhHcWgKjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "    product_name,\n",
        "    COUNT(*) AS total_orders,\n",
        "    SUM(reordered) AS total_reorders,\n",
        "    ROUND(100.0 * SUM(reordered) / COUNT(*), 2) AS reorder_rate\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY product_name\n",
        "ORDER BY reorder_rate DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Top 10 Products with Highest Reorder Rate:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "rFiEGj4egB6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights for Top 10 Products with Highest Reorder Rate:\n",
        "\n",
        "- **100% Lactose-Free Milk** has a **100% reorder rate**, indicating it is a highly demanded product among lactose-intolerant customers who consistently reorder it.\n",
        "- **Salted Sweet Cream Butter Quarters**, **Sparkling Water Bottles**, and **Premium Lots of Pulp Orange Juice** also show a **100% reorder rate**, signifying that these are popular products with strong customer loyalty and recurring demand.\n",
        "- **Classic Baby Creamers Potatoes** and **Peru Sweet Onions** are products with **100% reorder rate**, pointing to their continued preference and high re-purchase frequency among customers.\n",
        "- **Thirst Quencher Caffeine-Free Naturally Flavored Citrus Soda** and **Smoked Whitefish Salad** reflect **100% reorder rates**, highlighting that these items are consistently chosen by customers who keep coming back for more.\n",
        "- **Seltzer Water** and **Green Bananas** demonstrate high reorder demand, each maintaining **100% reorder rate**, suggesting these are staple items that customers rely on regularly."
      ],
      "metadata": {
        "id": "d5s45o0wgQnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orders by Department"
      ],
      "metadata": {
        "id": "BO9qFmH-gUqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT department, COUNT(*) AS total_orders\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY department\n",
        "ORDER BY total_orders DESC;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Orders by Department:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "032bVft_gSdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Bar Chart: Orders by Department\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Department, Total Orders)\n",
        "departments = [\"produce\", \"dairy eggs\", \"snacks\", \"beverages\", \"frozen\",\n",
        "               \"pantry\", \"bakery\", \"deli\", \"canned goods\", \"dry goods pasta\"]\n",
        "total_orders = [1116850, 619999, 294213, 234613, 201359, 138924, 127884, 114349, 102416, 84651]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_orders_by_department = pd.DataFrame({\"Department\": departments, \"Total Orders\": total_orders})\n",
        "\n",
        "# Plotting Bar Chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"Total Orders\", y=\"Department\", data=df_orders_by_department, palette=\"magma\")\n",
        "plt.xlabel(\"Total Orders\")\n",
        "plt.ylabel(\"Department\")\n",
        "plt.title(\"Total Orders by Department\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dKS9D6ghgZ_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights\n",
        "\n",
        "- Produce is the most ordered department with ~1.11 million orders. This aligns with the earlier Top Ordered Products (bananas, avocados, berries). On another hand, Fresh produce is frequently bought and likely reordered often.\n",
        "- Dairy & Eggs ranks second with 619K orders.\n",
        "- Snacks are the third most popular category (294K orders).\n",
        "    - Expect chips, granola bars, and nuts to dominate.\n",
        "- Beverages & Frozen Foods also have strong demand.\n",
        "    - Beverages (~234K orders) likely include popular items like bottled water, juices, and coffee.\n",
        "    - Frozen Foods (~201K orders) suggest customers stock up on frozen essentials.\n",
        "- Pantry Staples, Bakery, and Deli also contribute significantly. It could likely contain bread, canned goods, and dry pasta."
      ],
      "metadata": {
        "id": "-owCEChvgcpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Popular Aisles"
      ],
      "metadata": {
        "id": "sJf3yVVCgiSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT aisle, COUNT(*) AS total_orders\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY aisle\n",
        "ORDER BY total_orders DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Top 10 Aisles:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "XCJrBCHlgfe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Bar Chart: Most Popular Aisles\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Aisle, Total Orders)\n",
        "aisles = [\"fresh fruits\", \"fresh vegetables\", \"packaged vegetables fruits\", \"yogurt\",\n",
        "          \"packaged cheese\", \"milk\", \"water seltzer sparkling water\", \"chips pretzels\",\n",
        "          \"soy lactosefree\", \"bread\"]\n",
        "aisle_orders = [450026, 404252, 205492, 184903, 112690, 103953, 78019, 74536, 68786, 65469]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_aisles = pd.DataFrame({\"Aisle\": aisles, \"Total Orders\": aisle_orders})\n",
        "\n",
        "# Plotting Bar Chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"Total Orders\", y=\"Aisle\", data=df_aisles, palette=\"coolwarm\")\n",
        "plt.xlabel(\"Total Orders\")\n",
        "plt.ylabel(\"Aisle\")\n",
        "plt.title(\"Top 10 Most Popular Aisles\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DnPsE0x5gnJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights\n",
        "- Fresh Produce Dominance:\n",
        "    - Fresh Fruits (450K orders) and Fresh Vegetables (404K orders) are the top two aisles.\n",
        "    - Combined, these two alone account for over 950K orders, which reinforces why Produce is the top department.\n",
        "\n",
        "- Dairy is Highly Popular:\n",
        "    - Yogurt (184K orders) and Packaged Cheese (112K orders) show strong demand.\n",
        "    - Milk (103K orders) further confirms that dairy products are household essentials.\n",
        "\n",
        "- Beverages are a Major Category:\n",
        "    - Water, Seltzer, and Sparkling Water (78K orders) ranks #7, showing strong demand for bottled drinks.\n",
        "\n",
        "- Snacks & Bread are Key Pantry Items\n",
        "    - Chips & Pretzels (74K orders) are among the most frequently purchased snacks.\n",
        "    - Bread (65K orders) confirms why Bakery is among the top departments.\n",
        "\n",
        "- Plant-Based Alternatives are Growing\n",
        "    - Soy & Lactose-Free Products (68K orders) indicate increased demand for dairy-free alternatives."
      ],
      "metadata": {
        "id": "Fp_WsgZ9gomi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reorder Ratio by Department"
      ],
      "metadata": {
        "id": "Ni88HLrpgrk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "    department,\n",
        "    COUNT(*) AS total_orders,\n",
        "    SUM(reordered) AS total_reorders,\n",
        "    ROUND(100.0 * SUM(reordered) / COUNT(*), 2) AS reorder_ratio\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY department\n",
        "ORDER BY reorder_ratio DESC;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "print(\"Reorder Ratio by Department:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "gJ1IrcRYgqse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights\n",
        "\n",
        "- Dairy & Eggs Have the Highest Reorder Rate (82.73%)\n",
        "    - Most frequently reordered category.\n",
        "    - Milk, Yogurt, and Cheese are household staples → high repurchase behavior.\n",
        "\n",
        "- Beverages Rank #2 in Reorders (80.85%)\n",
        "    - Bottled Water, Sparkling Water, and Coffee/Tea are commonly repurchased.\n",
        "    - These items are frequently consumed & replaced regularly.\n",
        "\n",
        "- Produce Has a High Reorder Rate (80.71%)\n",
        "    - Fruits and vegetables have a high purchase frequency.\n",
        "    - Bananas, Avocados, and Berries from previous queries reinforce this trend.\n",
        "\n",
        "- Bakery (80.43%) & Deli (78.53%) Show Strong Reorder Loyalty\n",
        "    - Bread, Bagels, and Pre-packaged Deli Items are regularly bought items.\n",
        "    - Customers often stick to the same brands.\n",
        "\n",
        "- Snacks & Frozen Foods Have Moderate Reorder Rates (~70%)\n",
        "    - Chips, Pretzels, and Frozen Meals are repurchased but less frequently than fresh foods.\n",
        "\n",
        "- Canned Goods & Dry Goods Have Lower Reorder Rates (~65%)\n",
        "    - Longer shelf life → not purchased as frequently.\n",
        "\n",
        "- Pasta, sauces, and canned vegetables last longer → lower immediate repurchase need.\n",
        "\n",
        "- Pantry Has the Lowest Reorder Rate (57.26%)\n",
        "    - Less frequent purchases of pantry staples like flour, condiments, and spices."
      ],
      "metadata": {
        "id": "QJFJ6C8Tgwmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Pie Chart: Reorder Ratio by Department\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Department, Reorder Ratio)\n",
        "departments = [\"dairy eggs\", \"beverages\", \"produce\", \"bakery\", \"deli\",\n",
        "               \"snacks\", \"frozen\", \"canned goods\", \"dry goods pasta\", \"pantry\"]\n",
        "reorder_ratio = [82.73, 80.85, 80.71, 80.43, 78.53, 74.01, 71.87, 65.51, 65.33, 57.26]\n",
        "\n",
        "# Create DataFrame\n",
        "df_departments = pd.DataFrame({\"Department\": departments, \"Reorder Ratio\": reorder_ratio})\n",
        "\n",
        "# Plot Pie Chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(df_departments[\"Reorder Ratio\"], labels=df_departments[\"Department\"],\n",
        "        autopct=\"%1.1f%%\", colors=sns.color_palette(\"viridis\", len(departments)), startangle=140)\n",
        "plt.title(\"Reorder Ratio by Department\")\n",
        "plt.axis(\"equal\")  # Equal aspect ratio ensures the pie is drawn as a circle\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gHsh8pMwgzc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Stacked Bar Chart: Reordered vs Non-Reordered Orders by Department\n",
        "# ===========================\n",
        "\n",
        "# Data from previous query (Department, Reordered Orders)\n",
        "departments = [\"dairy eggs\", \"beverages\", \"produce\", \"bakery\", \"deli\",\n",
        "               \"snacks\", \"frozen\", \"canned goods\", \"dry goods pasta\", \"pantry\"]\n",
        "total_orders = [619999, 234613, 1116850, 127884, 114349, 294213, 201359, 102416, 84651, 138924]\n",
        "reordered = [512956, 189692, 901358, 102853, 89802, 217758, 144719, 67094, 55299, 79541]\n",
        "non_reordered = [total - reorder for total, reorder in zip(total_orders, reordered)]\n",
        "\n",
        "# Create DataFrame\n",
        "df_reorders = pd.DataFrame({\"Department\": departments, \"Reordered\": reordered, \"Non-Reordered\": non_reordered})\n",
        "\n",
        "# Plot Stacked Bar Chart (Reordered vs Non-Reordered)\n",
        "df_reorders.set_index(\"Department\")[[\"Reordered\", \"Non-Reordered\"]].plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"viridis\")\n",
        "plt.xlabel(\"Department\")\n",
        "plt.ylabel(\"Number of Orders\")\n",
        "plt.title(\"Reordered vs Non-Reordered Orders by Department\")\n",
        "plt.legend([\"Reordered\", \"Non-Reordered\"])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5WgzEX67g4M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis for reordered vs non-reordered orders by department**:\n",
        "    \n",
        "1. Produce has the highest number of orders overall\n",
        "\n",
        "    - Most of these orders are reorders, confirming that fruits and vegetables are frequently repurchased.\n",
        "\n",
        "2. Dairy & Eggs have the highest reorder percentage\n",
        "    - Consistently repurchased products like milk, cheese, and yogurt drive these numbers.\n",
        "\n",
        "3. Snacks and Frozen Foods have moderate reorder levels\n",
        "    - Customers repurchase snacks and frozen goods but at a slightly lower frequency than fresh items.\n",
        "\n",
        "4. Pantry and Canned Goods have the lowest reorder rates\n",
        "    - These products have a longer shelf life, reducing the need for frequent repurchasing."
      ],
      "metadata": {
        "id": "a4XgnpzPg5qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Analysis"
      ],
      "metadata": {
        "id": "ErNU5sapg_iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data: Numerical Features for Correlation Analysis\n",
        "departments = [\"dairy eggs\", \"beverages\", \"produce\", \"bakery\", \"deli\",\n",
        "               \"snacks\", \"frozen\", \"canned goods\", \"dry goods pasta\", \"pantry\"]\n",
        "total_orders = [619999, 234613, 1116850, 127884, 114349, 294213, 201359, 102416, 84651, 138924]\n",
        "reordered = [512956, 189692, 901358, 102853, 89802, 217758, 144719, 67094, 55299, 79541]\n",
        "reorder_ratio = [82.73, 80.85, 80.71, 80.43, 78.53, 74.01, 71.87, 65.51, 65.33, 57.26]\n",
        "\n",
        "# Creating DataFrame\n",
        "df_correlation = pd.DataFrame({  # Fix this correlation analysis to show up the departments.\n",
        "    \"Total Orders\": total_orders,\n",
        "    \"Reordered Orders\": reordered,\n",
        "    \"Reorder Ratio (%)\": reorder_ratio\n",
        "})\n",
        "\n",
        "# Computing Correlation Matrix\n",
        "correlation_matrix = df_correlation.corr()\n",
        "\n",
        "# Plotting Correlation Heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix of Order Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "80ZKtFbOg7tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Total Orders & Reordered Orders (Correlation = 1.00)\n",
        "    - This confirms that departments with high total orders also have high reorders.\n",
        "    - Produce, Dairy, and Beverages are likely contributing to this trend.\n",
        "\n",
        "- Total Orders & Reorder Ratio (Correlation = 0.49)\n",
        "    - Moderate positive correlation → Higher orders somewhat influence the reorder rate, but not always.\n",
        "    - Some departments may have high first-time purchases but lower reorder rates (e.g., snacks, pantry items).\n",
        "\n",
        "- Reordered Orders & Reorder Ratio (Correlation = 0.52)\n",
        "    - A moderate correlation suggests that higher reorder volumes influence reorder ratio but not perfectly.\n",
        "    - Some products (like dairy & produce) are reordered very frequently, while others (like pantry goods) less frequently."
      ],
      "metadata": {
        "id": "pJ0WRbr3hG7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of the training EDA"
      ],
      "metadata": {
        "id": "okQm0EpphJaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For creating ML Feature features, I am going to use the following labels:\n",
        "\n",
        "- User ID: \tThis tracks individual purchase behavior.\n",
        "- Product ID: Helps identify frequently reordered products.\n",
        "- Department ID: Some departments have higher reorder rates.\n",
        "- Aisle ID: Aisle-level trends impact reorder likelihood.\n",
        "- Total Orders: Highly correlated with reorder behavior.\n",
        "- Reorder Ratio: Strong predictor of repeat purchases.\n",
        "- Total items in Orders: \tDetermines if larger orders influence reorders.\n",
        "- User Order Frequency: Identifies frequent vs. occasional buyers.\n",
        "- User Reorder Percentage: Determines likelihood of repeat purchases.\n",
        "- Product popularity: Captures demand for the product.\n",
        "- Department reorder ratio: Some departments have stronger reorder trends.\n",
        "- Aisle Reorder ratio: Aisle-specific reorder behavior.\n",
        "- Product Reorder trend:  Helps detect seasonal or trending products."
      ],
      "metadata": {
        "id": "f5J3BAL8hOk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strongest predictions for EDA:\n",
        "\n",
        "1. Reorder Ratio: strongly correlated with reorders.\n",
        "2. Total Orders: high correlation with reorder likelihood.\n",
        "3. User Reorder percerntage: Helps preduct if a user is likely to reorder.\n",
        "4. Product Popularity: popular items have higher reorders.\n",
        "5. Department and Aisle reorder ratios: certain categories drive higher reorders."
      ],
      "metadata": {
        "id": "7_R9yd1NhQLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Store ( Aggregated Features)"
      ],
      "metadata": {
        "id": "04k79ecJhTdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan for Feature store:\n",
        "\n",
        "- We will definig the feature to store for:\n",
        "    - Product-level\n",
        "    - user-level\n",
        "    - department-level\n",
        "- Then, we will be creating the feature store in SageMaker\n",
        "    - Using boto3 and sagemaker.feature_store.feature_group\n",
        "- Next, we will ingest the engineered features\n",
        "    - We will be saving both offline (in S3 parquet format) and online ( real-time queryable)\n",
        "- Lastly, we will query the features from the Feature Store.\n",
        "    - We will retrieve features for training and inference."
      ],
      "metadata": {
        "id": "j_9-aAPchXs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Feture Store will base computed and aggregated based on the following features:\n",
        "\n",
        "- User-Based Features\n",
        "\n",
        "    - user_total_orders: total number of orders a user has placed\n",
        "    - user_reorder_ratio: percentage of the user’s past orders that contained reorders\n",
        "    - user_avg_items_per_order: average number of items per order for the user\n",
        "    \n",
        "- Product-Based Features\n",
        "\n",
        "    - product_total_orders: number of times the product was ordered overall\n",
        "    - product_reorder_ratio: how often the product gets reordered\n",
        "    - product_unique_users: number of unique users who ordered the product\n",
        "    \n",
        "- Department-Based Features\n",
        "\n",
        "    - department_reorder_ratio: average reorder ratio for all products in the department\n",
        "    - aisle_reorder_ratio: average reorder ratio for all products in the aisle"
      ],
      "metadata": {
        "id": "LG2yFpzZhaYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next code, I am going to aggregate user, product, and department-level features from the training dataset stored in Athena."
      ],
      "metadata": {
        "id": "vgiTsv7rhcSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Athena Database & Table\n",
        "database_name = \"instacart_db_split\"\n",
        "table_name = \"instacart_orders\"\n",
        "\n",
        "# ==========================\n",
        "# 🔹 1. Compute User-Level Features\n",
        "# ==========================\n",
        "user_query = f\"\"\"\n",
        "SELECT\n",
        "    user_id,\n",
        "    COUNT(DISTINCT order_id) AS user_total_orders,\n",
        "    SUM(reordered) / COUNT(*) AS user_reorder_ratio,\n",
        "    COUNT(*) / COUNT(DISTINCT order_id) AS user_avg_items_per_order\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY user_id;\n",
        "\"\"\"\n",
        "user_features = wr.athena.read_sql_query(user_query, database=database_name)\n",
        "\n",
        "# ==========================\n",
        "# 🔹 2. Compute Product-Level Features\n",
        "# ==========================\n",
        "product_query = f\"\"\"\n",
        "SELECT\n",
        "    product_id,\n",
        "    COUNT(*) AS product_total_orders,\n",
        "    SUM(reordered) / COUNT(*) AS product_reorder_ratio,\n",
        "    COUNT(DISTINCT user_id) AS product_unique_users\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY product_id;\n",
        "\"\"\"\n",
        "product_features = wr.athena.read_sql_query(product_query, database=database_name)\n",
        "\n",
        "# ==========================\n",
        "# 🔹 3. Compute Department & Aisle Features\n",
        "# ==========================\n",
        "department_query = f\"\"\"\n",
        "SELECT\n",
        "    department,\n",
        "    SUM(reordered) / COUNT(*) AS department_reorder_ratio\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY department;\n",
        "\"\"\"\n",
        "department_features = wr.athena.read_sql_query(department_query, database=database_name)\n",
        "\n",
        "aisle_query = f\"\"\"\n",
        "SELECT\n",
        "    aisle,\n",
        "    SUM(reordered) / COUNT(*) AS aisle_reorder_ratio\n",
        "FROM {database_name}.{table_name}\n",
        "GROUP BY aisle;\n",
        "\"\"\"\n",
        "aisle_features = wr.athena.read_sql_query(aisle_query, database=database_name)\n",
        "\n",
        "# ==========================\n",
        "# 🔹 4. Load Training Data to Get User-Product Mapping\n",
        "# ==========================\n",
        "mapping_query = f\"\"\"\n",
        "SELECT user_id, product_id, department, aisle\n",
        "FROM {database_name}.{table_name}\n",
        "\"\"\"\n",
        "df_mapping = wr.athena.read_sql_query(mapping_query, database=database_name)\n",
        "\n",
        "# ==========================\n",
        "# 🔹 5. Merge Features into a Single Dataset\n",
        "# ==========================\n",
        "df_features = (\n",
        "    df_mapping\n",
        "    .merge(user_features, on=\"user_id\", how=\"left\")  # Merge user features\n",
        "    .merge(product_features, on=\"product_id\", how=\"left\")  # Merge product features\n",
        "    .merge(department_features, on=\"department\", how=\"left\")  # Merge department features\n",
        "    .merge(aisle_features, on=\"aisle\", how=\"left\")  # Merge aisle features\n",
        ")\n",
        "\n",
        "print(\"Feature Engineering Completed Successfully!\")\n",
        "print(df_features.head())\n",
        "\n",
        "# ==========================\n",
        "# 🔹 6. Save Engineered Features to S3\n",
        "# ==========================\n",
        "parquet_output_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/features/\"\n",
        "wr.s3.to_parquet(\n",
        "    df=df_features,\n",
        "    path=parquet_output_path,\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "print(\"Feature dataset saved to S3!\")"
      ],
      "metadata": {
        "id": "eqm185KVhHjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional. Run the following code just in case the VM stops and you lose the connection with the Athena database. This code is only valid for Checking the existence of Feature Store that was made in the previous step"
      ],
      "metadata": {
        "id": "4gYQiZNihjSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note. Make sure to install the and import the necessary libraries first."
      ],
      "metadata": {
        "id": "Gt-K-yrAhlFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyathena import connect\n",
        "import awswrangler as wr\n",
        "\n",
        "# AWS Configuration\n",
        "bucket_name = \"sagemaker-us-east-1-921916832724\"\n",
        "region = \"us-east-1\"\n",
        "database_name = \"instacart_db_split\"\n",
        "table_name = \"instacart_orders\"\n",
        "\n",
        "# Define Athena Staging Directory\n",
        "s3_staging_dir = f\"s3://{bucket_name}/athena/instacart_staging_split/\"\n",
        "\n",
        "# Reconnect to Athena\n",
        "conn = connect(s3_staging_dir=s3_staging_dir, region_name=region)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "print(\"✅ Reconnected to Athena.\")"
      ],
      "metadata": {
        "id": "UMapqr94hnJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Step 2: Verify Partitions Exist\n",
        "#Since features are stored per department, let's check if partitions are loaded correctly:\n",
        "\n",
        "\n",
        "cursor.execute(f\"SHOW PARTITIONS {database_name}.{table_name}\")\n",
        "partitions = cursor.fetchall()\n",
        "\n",
        "if partitions:\n",
        "    print(f\"✅ Partitions found in table '{table_name}':\")\n",
        "    for partition in partitions:\n",
        "        print(partition)\n",
        "else:\n",
        "    print(f\"❌ No partitions found in table '{table_name}'.\")"
      ],
      "metadata": {
        "id": "EaHcA2-Khrqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Query Features for a Specific Department\n",
        "#Since features are partitioned by department, we must query them by department name:\n",
        "\n",
        "selected_department = \"bakery\"  # Change this to any department\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT *\n",
        "FROM {database_name}.{table_name}\n",
        "WHERE department = '{selected_department}'\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "print(f\"✅ Sample Data from '{selected_department}' Department:\")\n",
        "for row in rows:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "JlPYkIOKhvgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Verify Feature Files Exist in S3\n",
        "#Ensure feature files are actually in S3:\n",
        "\n",
        "import boto3\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "prefix = \"data-lake/Project/features/\"\n",
        "\n",
        "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "\n",
        "if 'Contents' in response:\n",
        "    print(\"✅ Feature files found in S3:\")\n",
        "    for obj in response['Contents']:\n",
        "        print(obj['Key'])\n",
        "else:\n",
        "    print(\"❌ No feature files found in S3!\")"
      ],
      "metadata": {
        "id": "5-snPbVVhxq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Load Features from S3\n",
        "# If features exist in S3, reload them using awswrangler:\n",
        "\n",
        "feature_path = f\"s3://{bucket_name}/data-lake/Project/features/\"\n",
        "\n",
        "df_features = wr.s3.read_parquet(feature_path)\n",
        "\n",
        "print(\"✅ Feature dataset loaded successfully!\")\n",
        "print(df_features.head())"
      ],
      "metadata": {
        "id": "sjJXllJchz2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging New Obtained Features into one single dataset"
      ],
      "metadata": {
        "id": "zxIUWxcNh32O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Since \"department\" is a parition column and it was not explicitly included in df_features. I am going to loop through all departments in the S3 feature store and concatenate them into a single DataFrame"
      ],
      "metadata": {
        "id": "3HfyexL5h1zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "\n",
        "# Listing of all departments\n",
        "departments = [\n",
        "    \"bakery\", \"beverages\", \"canned goods\", \"dairy eggs\", \"deli\",\n",
        "    \"dry goods pasta\", \"frozen\", \"pantry\", \"produce\", \"snacks\"\n",
        "]\n",
        "\n",
        "# S3 base path\n",
        "s3_base_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/features/\"\n",
        "\n",
        "# Loading and merging all department data\n",
        "df_list = []\n",
        "\n",
        "for dept in departments:\n",
        "    s3_path = f\"{s3_base_path}department={dept}/\"\n",
        "\n",
        "    # Loading department-specific features\n",
        "    df_dept = wr.s3.read_parquet(path=s3_path)\n",
        "\n",
        "    # Adding department column back\n",
        "    df_dept[\"department\"] = dept\n",
        "\n",
        "    df_list.append(df_dept)\n",
        "\n",
        "# Merging all departments into a single DataFrame\n",
        "df_all_departments = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Displaying result\n",
        "print(\"✅ Successfully loaded all department features!\")\n",
        "print(df_all_departments.head())"
      ],
      "metadata": {
        "id": "rJTn8v4rh9C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have successfully merged all department features, I am going to save the merged dataset back to S3\n",
        "\n",
        "# Defining output path for merged dataset\n",
        "merged_output_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/merged_features/\"\n",
        "\n",
        "# Saving merged features back to S3 (partitioned by department)\n",
        "wr.s3.to_parquet(\n",
        "    df=df_all_departments,\n",
        "    path=merged_output_path,\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "print(\" Merged feature dataset saved to S3!\")"
      ],
      "metadata": {
        "id": "iviYoJLNiCr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (EDA) on the Merged Dataset (Aggregated Feature)"
      ],
      "metadata": {
        "id": "0xJQFgV0iFOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking  the distribution of orders across departments\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(y=df_all_departments[\"department\"], order=df_all_departments[\"department\"].value_counts().index)\n",
        "plt.xlabel(\"Total Orders\")\n",
        "plt.ylabel(\"Department\")\n",
        "plt.title(\"Total Orders by Department\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1nk7ho7biFpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the Feature Store (Aggregated)  for Model Training"
      ],
      "metadata": {
        "id": "VSOiO485iJKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now have product-level, user-level, and department-level features. Next, We will structure the dataset for model training."
      ],
      "metadata": {
        "id": "5KifAnaRiJpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting relevant features for training\n",
        "features = [\n",
        "    \"user_total_orders\", \"user_reorder_ratio\", \"user_avg_items_per_order\",\n",
        "    \"product_total_orders\", \"product_reorder_ratio\", \"product_unique_users\",\n",
        "    \"department_reorder_ratio\", \"aisle_reorder_ratio\"\n",
        "]\n",
        "\n",
        "df_training = df_all_departments[features]\n",
        "\n",
        "# Saving training-ready dataset\n",
        "training_output_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/training_data/\"\n",
        "\n",
        "wr.s3.to_parquet(\n",
        "    df=df_training,\n",
        "    path=training_output_path,\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "print(\"✅ Training-ready dataset saved to S3!\")"
      ],
      "metadata": {
        "id": "0Pp2fvRTiMtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Phase"
      ],
      "metadata": {
        "id": "yQguC6afiQLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Model using the original Dataset"
      ],
      "metadata": {
        "id": "pY8i_no5iTWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading required libraries"
      ],
      "metadata": {
        "id": "bEpmdBEHiVuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required libraries\n",
        "!pip install scikit-learn boto3 awswrangler pandas numpy\n",
        "\n",
        "# Importing libraries\n",
        "import boto3\n",
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "GL6t7iRBiRBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Raw Training and  Validation Dataset"
      ],
      "metadata": {
        "id": "P1R8euTeiZj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining S3 paths for raw datasets\n",
        "raw_train_s3_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/train/\"\n",
        "raw_validation_s3_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/validation/\"\n",
        "\n",
        "# Loading raw training and validation datasets\n",
        "df_train_raw = wr.s3.read_parquet(raw_train_s3_path)\n",
        "df_validation_raw = wr.s3.read_parquet(raw_validation_s3_path)\n",
        "\n",
        "# Checking if datasets loaded correctly\n",
        "print(\" Raw Training Dataset Loaded Successfully!\")\n",
        "print(df_train_raw.head())\n",
        "\n",
        "print(\" Raw Validation Dataset Loaded Successfully!\")\n",
        "print(df_validation_raw.head())"
      ],
      "metadata": {
        "id": "vZwgbpDVibVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Logistic Regression on Raw Data"
      ],
      "metadata": {
        "id": "u4rU4nSmid-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "#  Loading Raw Training Data\n",
        "raw_train_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/train/\"\n",
        "df_raw = wr.s3.read_parquet(raw_train_path)\n",
        "\n",
        "#  Selecting Features & Target\n",
        "X_raw = df_raw[[\"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]]\n",
        "y_raw = df_raw[\"reordered\"]\n",
        "\n",
        "#  Split Data into Train and Validation Sets\n",
        "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)\n",
        "\n",
        "#  Normalize Data (Optional)\n",
        "scaler = StandardScaler()\n",
        "X_train_raw_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_val_raw_scaled = scaler.transform(X_val_raw)\n",
        "\n",
        "#  Train Logistic Regression Model on Raw Data\n",
        "log_reg_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg_raw.fit(X_train_raw_scaled, y_train_raw)\n",
        "\n",
        "#  Predictions on Validation Set\n",
        "y_pred_raw = log_reg_raw.predict(X_val_raw_scaled)\n",
        "\n",
        "#  Evaluating Model\n",
        "print(\" Logistic Regression Performance on Raw Data:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val_raw, y_pred_raw))\n",
        "print(classification_report(y_val_raw, y_pred_raw))"
      ],
      "metadata": {
        "id": "XknSXVJvig2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Observations from Logistic Regression on RAW Data**\n",
        "\n",
        "    - Accuracy: 77.75%\n",
        "    - F1-Score (Weighted Avg): 68%\n",
        "    - Precision for Class 0 is 0.00  (Model is not predicting non-reorders at all)\n",
        "    - Recall for Class 1 is 1.00, which means the model predicts almost everything as reordered (1).\n",
        "\n",
        "- Issues with the Model\n",
        "    - Severe Class Imbalance:\n",
        "        - The dataset has 471,999 reordered (1) vs. 135,053 not reordered (0).\n",
        "        - The model is likely biased toward predicting everything as reordered.\n",
        "\n",
        "- Zero Precision for Class 0:\n",
        "The model never predicts non-reorders, meaning it completely ignores that class.\n",
        "\n",
        "High Recall but Low Precision:\n",
        "It catches most reorders but makes many false positives.\n",
        "Not useful if we want accurate reorder predictions."
      ],
      "metadata": {
        "id": "hVBpu2sUikYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Updated Logistic Regression (With Class Weights)"
      ],
      "metadata": {
        "id": "-geCbiCZio_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Logistic Regression with Class Weights\n",
        "log_reg_weighted = LogisticRegression(max_iter=1000, random_state=42, class_weight=\"balanced\")\n",
        "log_reg_weighted.fit(X_train_raw_scaled, y_train_raw)\n",
        "\n",
        "# Predictions\n",
        "y_pred_weighted = log_reg_weighted.predict(X_val_raw_scaled)\n",
        "\n",
        "# Evaluate Model\n",
        "print(\"🔹 Logistic Regression (Balanced Classes) Performance on Raw Data:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val_raw, y_pred_weighted))\n",
        "print(classification_report(y_val_raw, y_pred_weighted))"
      ],
      "metadata": {
        "id": "vMUM3Bp_imoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Observations from Logistic Regression (Balanced Classes) on RAW Data**\n",
        "\n",
        "\n",
        "- Accuracy: 54.9%  (Dropped from 77.75%)\n",
        "- Precision (Class 0): 24% (Now at least predicting some non-reorders)\n",
        "- Recall (Class 0): 48% (Improved from 0%)\n",
        "- Precision (Class 1): 79%  (Dropped, but still decent)\n",
        "- Recall (Class 1): 57%  (Worse than before, but balanced)\n",
        "- Weighted F1-Score: 59%  (More balanced than before)"
      ],
      "metadata": {
        "id": "mh3yZLU3itan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Takeaways**\n",
        "\n",
        "- Improvement:\n",
        "\n",
        "    - The model now predicts some non-reorders (0), which is a step forward.\n",
        "    - The recall for non-reorders (48%) is better than before (0%).\n",
        "\n",
        "- Trade-offs:\n",
        "\n",
        "- Accuracy dropped (because the model is now making more mistakes overall)\n",
        "- Class 1 (Reorders) recall fell from 100% → 57%.\n",
        "\n",
        "Better balance overall, but not ideal. The model still struggles with predicting non-reorders accurately."
      ],
      "metadata": {
        "id": "IvWyFJQWiu_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Model using the Feature Store Data (Aggregated)"
      ],
      "metadata": {
        "id": "OjRCbF8mixuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Feature-Engineered Dataset"
      ],
      "metadata": {
        "id": "8Bi6f8Ipizz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding \"reordered column back to the merged-feature-engineered-dataset\". This step is done since I forgot to add the target column to the feature engineering agreggated dataset."
      ],
      "metadata": {
        "id": "x_B-euB0it6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "\n",
        "# Loading the Original Training Dataset (Partitioned by Department)\n",
        "original_train_s3_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/train/\"\n",
        "df_train = wr.s3.read_parquet(original_train_s3_path, columns=[\"user_id\", \"product_id\", \"reordered\", \"department\", \"department_id\"])\n",
        "\n",
        "print(\" Original Training Dataset Loaded Successfully!\")\n",
        "print(df_train.head())\n",
        "\n",
        "# Loading the Feature Store Dataset (Without `reordered`)\n",
        "feature_store_s3_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/merged_features/\"\n",
        "df_features = wr.s3.read_parquet(feature_store_s3_path)\n",
        "\n",
        "print(\" Feature Store Dataset Loaded Successfully!\")\n",
        "print(df_features.head())\n",
        "\n",
        "# Verifying Column Names\n",
        "print(\"Columns in df_train:\", df_train.columns)\n",
        "print(\"Columns in df_features:\", df_features.columns)\n",
        "\n",
        "# Merging `reordered` and `department` Column Back\n",
        "df_updated_features = df_features.merge(df_train, on=[\"user_id\", \"product_id\"], how=\"left\")\n",
        "\n",
        "# If department is missing, check for `department_id`\n",
        "if \"department\" not in df_updated_features.columns and \"department_id\" in df_updated_features.columns:\n",
        "    # Mapping department_id to department name (if needed)\n",
        "    department_mapping = {\n",
        "        1: \"frozen\", 2: \"other\", 3: \"bakery\", 4: \"produce\", 5: \"alcohol\",\n",
        "        6: \"international\", 7: \"beverages\", 8: \"pets\", 9: \"dry goods pasta\",\n",
        "        10: \"bulk\", 11: \"personal care\", 12: \"meat seafood\", 13: \"pantry\",\n",
        "        14: \"breakfast\", 15: \"canned goods\", 16: \"dairy eggs\", 17: \"household\",\n",
        "        18: \"babies\", 19: \"snacks\", 20: \"deli\", 21: \"missing\"\n",
        "    }\n",
        "    df_updated_features[\"department\"] = df_updated_features[\"department_id\"].map(department_mapping)\n",
        "\n",
        "print(\" Successfully Merged `reordered` and `department` Columns!\")\n",
        "print(df_updated_features.head())\n",
        "\n",
        "# Verifying \"department\" Column Exists Before Saving\n",
        "if \"department\" not in df_updated_features.columns:\n",
        "    raise ValueError(\"❌ ERROR: The `department` column is STILL missing from the merged dataset!\")\n",
        "\n",
        "# Saving the Updated Dataset with `reordered` and `department`\n",
        "updated_feature_store_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/features_with_reorder/\"\n",
        "wr.s3.to_parquet(\n",
        "    df=df_updated_features,\n",
        "    path=updated_feature_store_path,\n",
        "    dataset=True,\n",
        "    mode=\"overwrite\",\n",
        "    partition_cols=[\"department\"],  # Ensuring correct partitioning\n",
        "    compression=\"snappy\"\n",
        ")\n",
        "\n",
        "print(\" Feature Store Dataset with `reordered` and `department` Column Saved Successfully!\")"
      ],
      "metadata": {
        "id": "yZyPZdUHi4af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Logistic Regression on Feature-Engineered Data"
      ],
      "metadata": {
        "id": "GVbJkhF1i89I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "#  Loading Feature Engineered Training Data\n",
        "feature_store_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/features_with_reorder/\"\n",
        "df_features = wr.s3.read_parquet(feature_store_path)\n",
        "\n",
        "#  Selecting Features & Target (Keeping `department_id`)\n",
        "X_features = df_features.drop(columns=[\"user_id\", \"product_id\", \"reordered\"])\n",
        "y_features = df_features[\"reordered\"]\n",
        "\n",
        "#  Converting Categorical Column \"aisle\" to Numerical (Label Encoding)\n",
        "label_encoder = LabelEncoder()\n",
        "X_features[\"aisle\"] = label_encoder.fit_transform(X_features[\"aisle\"])\n",
        "\n",
        "#  Splitting Train-Validation Set\n",
        "X_train_features, X_val_features, y_train_features, y_val_features = train_test_split(\n",
        "    X_features, y_features, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "#  Normalizing Features\n",
        "scaler = StandardScaler()\n",
        "X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
        "X_val_features_scaled = scaler.transform(X_val_features)\n",
        "\n",
        "#  Training Logistic Regression on Feature Store Data\n",
        "log_reg_features = LogisticRegression(max_iter=1000, random_state=42, class_weight=\"balanced\")\n",
        "log_reg_features.fit(X_train_features_scaled, y_train_features)\n",
        "\n",
        "#  Predictions\n",
        "y_pred_features = log_reg_features.predict(X_val_features_scaled)\n",
        "\n",
        "#  Evaluation\n",
        "print(\"🔹 Logistic Regression Performance on Feature Store Data:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val_features, y_pred_features))\n",
        "print(classification_report(y_val_features, y_pred_features))"
      ],
      "metadata": {
        "id": "iMzmgpnyi-oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Observations**\n",
        "\n",
        "\n",
        "- Higher Precision on Feature Store Model:\n",
        "\n",
        "    - The model trained on feature-engineered data has much better precision (0.94) for predicting reordered products compared to raw data (0.79).\n",
        "    - This means fewer false positives—when the model predicts a reorder, it's more likely to be correct.\n",
        "\n",
        "- Lower Recall on Feature Store Model:\n",
        "\n",
        "    - Recall dropped from 0.57 (raw) to 0.50 (features).\n",
        "    - This suggests the model is missing some reorders, likely because of the feature transformations.\n",
        "\n",
        "- Similar F1-Score:\n",
        "\n",
        "    - Despite recall decreasing, the F1-score remains the same (0.66), meaning overall predictive performance is stable.\n",
        "\n",
        "- ccuracy Decreased:\n",
        "\n",
        "    - Feature Store Model: 51.7%\n",
        "    - Raw Data Model: 54.9%\n",
        "\n",
        "The drop in accuracy suggests that feature transformations changed the class balance, but it’s expected since we engineered new feature sets."
      ],
      "metadata": {
        "id": "N-RszNzQjBxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I am going to try XGBoost. Which is efficient and handles large datasets well. It is powerful for imbalanced datasets and works well with both raw and engineered features.\n",
        "\n",
        "I am going to apply XGBoost in the RAW dataset first to later apply it in the Feature Store dataset."
      ],
      "metadata": {
        "id": "JIgu758njK1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with XGBoost on Raw Dataset"
      ],
      "metadata": {
        "id": "0kWpGwrGjQwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "RFERNoV3jFHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Training & Validation Data\n",
        "raw_train_s3_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/train/\"\n",
        "raw_validation_s3_path = \"s3://sagemaker-us-east-1-921916832724/data-lake/Project/partitioned_split/validation/\"\n",
        "\n",
        "df_train_raw = wr.s3.read_parquet(raw_train_s3_path)\n",
        "df_validation_raw = wr.s3.read_parquet(raw_validation_s3_path)\n",
        "\n",
        "print(\"✅ Raw Training & Validation Data Loaded!\")\n",
        "\n",
        "# Selecting Features & Target\n",
        "X_train_raw = df_train_raw[[\"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]]\n",
        "y_train_raw = df_train_raw[\"reordered\"]\n",
        "X_val_raw = df_validation_raw[[\"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]]\n",
        "y_val_raw = df_validation_raw[\"reordered\"]\n",
        "\n",
        "# 🔹 **Fix Label Encoding Issue**\n",
        "label_encoder_aisle = LabelEncoder()\n",
        "label_encoder_department = LabelEncoder()\n",
        "\n",
        "# Fit label encoders on combined data (train + validation) to prevent unseen labels issue\n",
        "all_aisle_ids = pd.concat([X_train_raw[\"aisle_id\"], X_val_raw[\"aisle_id\"]])\n",
        "all_department_ids = pd.concat([X_train_raw[\"department_id\"], X_val_raw[\"department_id\"]])\n",
        "\n",
        "label_encoder_aisle.fit(all_aisle_ids)\n",
        "label_encoder_department.fit(all_department_ids)\n",
        "\n",
        "# Transform the train and validation sets using the fitted encoders\n",
        "X_train_raw[\"aisle_id\"] = label_encoder_aisle.transform(X_train_raw[\"aisle_id\"])\n",
        "X_train_raw[\"department_id\"] = label_encoder_department.transform(X_train_raw[\"department_id\"])\n",
        "X_val_raw[\"aisle_id\"] = label_encoder_aisle.transform(X_val_raw[\"aisle_id\"])\n",
        "X_val_raw[\"department_id\"] = label_encoder_department.transform(X_val_raw[\"department_id\"])\n",
        "\n",
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_raw_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_val_raw_scaled = scaler.transform(X_val_raw)\n",
        "\n",
        "# Train XGBoost Model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)\n",
        "xgb_model.fit(X_train_raw_scaled, y_train_raw)\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = xgb_model.predict(X_val_raw_scaled)\n",
        "\n",
        "# Evaluate Model\n",
        "print(\"✅ XGBoost Performance on Raw Data:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val_raw, y_pred_xgb))\n",
        "print(classification_report(y_val_raw, y_pred_xgb))"
      ],
      "metadata": {
        "id": "aKUXFEROjVAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Model Results Analysis\n",
        " - The XGBoost model on raw data achieved 77.8% accuracy, but we need to analyze the imbalance further:\n",
        "\n",
        "    - Precision (0.56) for Class 0: Indicates that many of the predicted non-reorders (0) were false positives.\n",
        "    - Recall (0.02) for Class 0: Very low, meaning the model struggles to detect non-reorders (0).\n",
        "    - Recall (1.00) for Class 1: The model classifies most orders as reorders (1), likely because of class imbalance."
      ],
      "metadata": {
        "id": "B9ymXClbjWi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting Scale_pos_weight, since reorder=1 is much more frequent, we should adjust the balance"
      ],
      "metadata": {
        "id": "At-_70x9japt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the imbalance ratio:\n",
        "\n",
        "This is computed as:\n",
        "- scale_pos_weight = count of class 0 (non-reorders) / count of class 1 (reorders)"
      ],
      "metadata": {
        "id": "QYkHAoGKjc-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "#  Step 1: Compute scale_pos_weight\n",
        "class_counts = Counter(y_train_raw)\n",
        "scale_pos_weight = class_counts[0] / class_counts[1]\n",
        "print(f\"Computed scale_pos_weight: {scale_pos_weight:.4f}\")\n",
        "\n",
        "# Step 2: Train XGBoost with the new weight\n",
        "xgb_model_weighted = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=scale_pos_weight,  # Apply the weight\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model_weighted.fit(X_train_raw_scaled, y_train_raw)\n",
        "\n",
        "# Step 3: Predictions\n",
        "y_pred_weighted = xgb_model_weighted.predict(X_val_raw_scaled)\n",
        "\n",
        "# Step 4: Evaluate Performance\n",
        "print(\"\\n🔹 XGBoost Performance with scale_pos_weight:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val_raw, y_pred_weighted))\n",
        "print(classification_report(y_val_raw, y_pred_weighted))"
      ],
      "metadata": {
        "id": "kNNR1HH6jYsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Observations**:\n",
        "\n",
        "- Recall for Class 0 (Non-reorders) jumped from 2% ➝ 65%\n",
        "- Recall for Class 1 (Reorders) dropped from 100% ➝ 56% (expected tradeoff)\n",
        "- Overall Accuracy: 57.76% (slightly lower than before, but recall is better balanced)"
      ],
      "metadata": {
        "id": "u3XzmkqWjg3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning XGBoost on Raw Training Dataset"
      ],
      "metadata": {
        "id": "kG_E9TszjkoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model_tuned = xgb.XGBClassifier(\n",
        "    n_estimators=200,  # Increasing trees for better learning\n",
        "    max_depth=8,       # Allowing deeper splits\n",
        "    learning_rate=0.05, # Reducing step size\n",
        "    scale_pos_weight=0.4,  # Adjusting class balancing weight\n",
        "    eval_metric=\"auc\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model_tuned.fit(X_train_raw_scaled, y_train_raw)\n",
        "y_pred_tuned = xgb_model_tuned.predict(X_val_raw_scaled)\n",
        "\n",
        "# Evaluating\n",
        "print(\"\\n🔹 Fine-Tuned XGBoost Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val_raw, y_pred_tuned))\n",
        "print(classification_report(y_val_raw, y_pred_tuned))"
      ],
      "metadata": {
        "id": "m7QYAveqjit3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Key Takeaways from Fine-Tuned XGBoost**:\n",
        "    - Accuracy: 73.57% (Improved from 57.76%)\n",
        "    - Recall for Class 0 (Non-reorders): 30% (Better than 2% before)\n",
        "    - Recall for Class 1 (Reorders): 86% (Still strong)\n",
        "    - Precision & F1-score improved for both classes"
      ],
      "metadata": {
        "id": "3pRP85RhjnUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splice Point of Two Notebooks\n",
        "Can also be used as a checkpoint."
      ],
      "metadata": {
        "id": "EwGSXagVjxNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: this notebook is the continue of the previous notebook: \"InstantCart_Project_GXboost_Model_Group_Training-Batch-Time-Inference\"\n",
        "On this notebook, I fixing the Batch time inference Job to use a new approach:\n",
        "    - input_filter: * Set __input_filter__ to \"$[1:]\": indicates that we are excluding column 0 (the 'ID') before processing the inferences and keeping everything from column 1 to the last column (all the features or predictors)  \n",
        "    - join_source: * Set __join_source__ to \"Input\": indicates our desire to join the input data with the inference results  \n",
        "    - output_filter: Leave __output_filter__ to default ('$'), indicating that the joined input and inference results be will saved as output."
      ],
      "metadata": {
        "id": "1pdOgNMOj89f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "\n",
        "ray.init(object_store_memory=2 * 1024 * 1024 * 1024)  # 2GB memory"
      ],
      "metadata": {
        "id": "IykWm0kpjqI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install awswrangler"
      ],
      "metadata": {
        "id": "NNxLfxFvkCyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries"
      ],
      "metadata": {
        "id": "BkAaeyXrkGC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import sagemaker\n",
        "import boto3\n",
        "import joblib\n",
        "import tarfile\n",
        "from sagemaker import image_uris, get_execution_role, session\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "5t6RVOQ8kGdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining AWS Resources"
      ],
      "metadata": {
        "id": "AmKJjhRgkJHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region = boto3.Session().region_name\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = get_execution_role()\n",
        "s3_bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "parquet_output_path = f\"s3://{s3_bucket}/data-lake/Project/partitioned_split/\""
      ],
      "metadata": {
        "id": "mUx3ap9OkLOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Processing Pipeline for ALL Datasets"
      ],
      "metadata": {
        "id": "5J27-M4zkNsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reloading All Datasets"
      ],
      "metadata": {
        "id": "wq4nLNqnkPsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "\n",
        "# Defining S3 paths for the original full datasets\n",
        "train_csv_full_path = \"s3://sagemaker-us-east-1-921916832724/Project/train_data_full.csv\"\n",
        "validation_csv_full_path = \"s3://sagemaker-us-east-1-921916832724/Project/validation_data_full.csv\"\n",
        "test_csv_full_path = \"s3://sagemaker-us-east-1-921916832724/Project/test_data_full.csv\"\n",
        "production_csv_full_path = \"s3://sagemaker-us-east-1-921916832724/Project/production_data_full.csv\"\n",
        "\n",
        "# Reloading all datasets from S3\n",
        "df_train = wr.s3.read_csv(train_csv_full_path)\n",
        "df_validation = wr.s3.read_csv(validation_csv_full_path)\n",
        "df_test = wr.s3.read_csv(test_csv_full_path)\n",
        "df_production = wr.s3.read_csv(production_csv_full_path)\n",
        "\n",
        "print(\" Train, Validation, Test & Production datasets reloaded successfully!\")"
      ],
      "metadata": {
        "id": "U8zbWLGgkNFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applying Label Encoding and Scaling"
      ],
      "metadata": {
        "id": "6DLBPbEokVW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# **Step 1: Reinitialize Label Encoders**\n",
        "label_encoder_aisle = LabelEncoder()\n",
        "label_encoder_department = LabelEncoder()\n",
        "\n",
        "# **Step 2: Fit encoders using training & validation data**\n",
        "all_aisle_ids = pd.concat([df_train[\"aisle_id\"], df_validation[\"aisle_id\"]])\n",
        "all_department_ids = pd.concat([df_train[\"department_id\"], df_validation[\"department_id\"]])\n",
        "\n",
        "label_encoder_aisle.fit(all_aisle_ids)\n",
        "label_encoder_department.fit(all_department_ids)\n",
        "\n",
        "print(\" Label Encoders trained on training + validation data!\")\n",
        "\n",
        "# **Step 3: Apply label encoding to all datasets**\n",
        "for df in [df_train, df_validation, df_test, df_production]:\n",
        "    df[\"aisle_id\"] = label_encoder_aisle.transform(df[\"aisle_id\"])\n",
        "    df[\"department_id\"] = label_encoder_department.transform(df[\"department_id\"])\n",
        "\n",
        "print(\" Label Encoding applied to all datasets!\")\n",
        "\n",
        "# **Step 4: Standardize numerical features**\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit only on training dataset\n",
        "scaler.fit(df_train[[\"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]])\n",
        "\n",
        "# Apply the same scaling to all datasets\n",
        "for df in [df_train, df_validation, df_test, df_production]:\n",
        "    df[[\"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]] = scaler.transform(\n",
        "        df[[\"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]]\n",
        "    )\n",
        "\n",
        "print(\" Standardization applied to all datasets!\")"
      ],
      "metadata": {
        "id": "xo8SyG0tkVvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensuring the correct order of columns"
      ],
      "metadata": {
        "id": "CFxTmIRFkYzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure correct column order & remove headers**\n",
        "train_transformed = df_train[[\"reordered\", \"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]].to_numpy()\n",
        "val_transformed = df_validation[[\"reordered\", \"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]].to_numpy()\n",
        "test_transformed = df_test[[\"reordered\", \"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]].to_numpy()\n",
        "prod_transformed = df_production[[\"user_id\", \"product_id\", \"aisle_id\", \"department_id\"]].to_numpy()\n",
        "\n",
        "print(\" Column order corrected for all datasets!\")"
      ],
      "metadata": {
        "id": "xmaG-LBKkaj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting to Numpy ( Removing headers) and saving back to S3"
      ],
      "metadata": {
        "id": "SpyKD4iekdst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save transformed datasets back to S3 (No headers)**\n",
        "wr.s3.to_csv(pd.DataFrame(train_transformed),\n",
        "             path=\"s3://sagemaker-us-east-1-921916832724/Project/train_data_transformed.csv\",\n",
        "             index=False,\n",
        "             header=False)\n",
        "\n",
        "wr.s3.to_csv(pd.DataFrame(val_transformed),\n",
        "             path=\"s3://sagemaker-us-east-1-921916832724/Project/validation_data_transformed.csv\",\n",
        "             index=False,\n",
        "             header=False)\n",
        "\n",
        "wr.s3.to_csv(pd.DataFrame(test_transformed),\n",
        "             path=\"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed.csv\",\n",
        "             index=False,\n",
        "             header=False)\n",
        "\n",
        "wr.s3.to_csv(pd.DataFrame(prod_transformed),\n",
        "             path=\"s3://sagemaker-us-east-1-921916832724/Project/production_data_transformed.csv\",\n",
        "             index=False,\n",
        "             header=False)\n",
        "\n",
        "print(\" All datasets transformed and saved successfully!\")"
      ],
      "metadata": {
        "id": "txsedcopkegW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final checking of the datasets"
      ],
      "metadata": {
        "id": "Rwj2433skiVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking dataset shapes\n",
        "print(\"Transformed Dataset Shapes:\")\n",
        "print(f\"Train Transformed: {train_transformed.shape}\")\n",
        "print(f\"Validation Transformed: {val_transformed.shape}\")\n",
        "print(f\"Test Transformed: {test_transformed.shape}\")\n",
        "print(f\"Production Transformed: {prod_transformed.shape}\")\n",
        "\n",
        "# Checking first few rows of each dataset\n",
        "print(\"\\nSample Data from Transformed Train Set:\")\n",
        "print(train_transformed)\n",
        "\n",
        "print(\"\\nSample Data from Transformed Validation Set:\")\n",
        "print(val_transformed)\n",
        "\n",
        "print(\"\\nSample Data from Transformed Test Set:\")\n",
        "print(test_transformed)\n",
        "\n",
        "print(\"\\nSample Data from Transformed Production Set:\")\n",
        "print(prod_transformed)"
      ],
      "metadata": {
        "id": "2mfGxItmkjyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch inference Job"
      ],
      "metadata": {
        "id": "wOtvYpkekoAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.transformer import Transformer\n",
        "\n",
        "# Defining S3 paths\n",
        "test_data_s3_path = \"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed.csv\"\n",
        "batch_output_s3_path = \"s3://sagemaker-us-east-1-921916832724/Project/batch_predictions/\"\n",
        "\n",
        "# Defining the transformer for batch inference\n",
        "transformer = Transformer(\n",
        "    model_name=\"sagemaker-xgboost-2025-02-15-06-32-39-868\",  # Ensure this matches your registered model name\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    output_path=batch_output_s3_path\n",
        ")\n",
        "\n",
        "# Aligning with the Correct SageMaker Format\n",
        "transformer.assemble_with = \"Line\"\n",
        "transformer.accept = \"text/csv\"\n",
        "\n",
        "\n",
        "transformer.transform(\n",
        "    data=test_data_s3_path,\n",
        "    split_type=\"Line\",\n",
        "    content_type=\"text/csv\",\n",
        "    input_filter=\"$[1:]\",  #  JSONPath syntax\n",
        "    join_source=\"Input\"  # Associate input data with predictions\n",
        ")\n",
        "\n",
        "# Waiting for completion\n",
        "transformer.wait()\n",
        "print(f\" Batch predictions completed! Output saved at: {batch_output_s3_path}\")"
      ],
      "metadata": {
        "id": "W_DAu_DVkofh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations results on XGBoost (Batch Inference on Test Data)"
      ],
      "metadata": {
        "id": "aL-AVamqktLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "\n",
        "# AWS S3 Configuration\n",
        "s3_client = boto3.client(\"s3\")\n",
        "bucket_name = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_predictions_path = \"Project/batch_predictions/test_data_transformed.csv.out\"\n",
        "\n",
        "# Local path to store downloaded predictions\n",
        "batch_predictions_local_path = \"/tmp/test_data_transformed_predictions.csv\"\n",
        "\n",
        "# Downloading the Predictions File from S3\n",
        "s3_client.download_file(bucket_name, s3_predictions_path, batch_predictions_local_path)\n",
        "print(f\" Batch predictions downloaded to: {batch_predictions_local_path}\")\n",
        "\n",
        "# Loading Predictions and Actual Test Labels\n",
        "predictions_df = pd.read_csv(batch_predictions_local_path, header=None)\n",
        "predictions = predictions_df.iloc[:, -1].values  # Last column contains predictions\n",
        "\n",
        "# Loading the original test dataset to get actual labels\n",
        "s3_test_data_path = \"Project/test_data_transformed.csv\"\n",
        "test_data_local_path = \"/tmp/test_data_transformed.csv\"\n",
        "s3_client.download_file(bucket_name, s3_test_data_path, test_data_local_path)\n",
        "\n",
        "# Load test dataset\n",
        "test_df = pd.read_csv(test_data_local_path, header=None)\n",
        "actual_labels = test_df.iloc[:, 0].values\n",
        "\n",
        "# Ensuring the shape matches before evaluation\n",
        "if len(predictions) != len(actual_labels):\n",
        "    raise ValueError(f\"Shape mismatch: Predictions ({len(predictions)}) vs Labels ({len(actual_labels)})\")\n",
        "\n",
        "# Converting predictions to binary (0 or 1) based on a threshold of 0.5\n",
        "binary_predictions = np.where(predictions >= 0.5, 1, 0)\n",
        "\n",
        "# Computing Metrics\n",
        "accuracy = accuracy_score(actual_labels, binary_predictions)\n",
        "precision = precision_score(actual_labels, binary_predictions)\n",
        "recall = recall_score(actual_labels, binary_predictions)\n",
        "f1 = f1_score(actual_labels, binary_predictions)\n",
        "auc_roc = roc_auc_score(actual_labels, predictions)  # Use raw probabilities for AUC\n",
        "\n",
        "# Displaying Metrics\n",
        "print(\"\\n **Evaluation Results:**\")\n",
        "print(f\" Accuracy: {accuracy:.4f}\")\n",
        "print(f\" Precision: {precision:.4f}\")\n",
        "print(f\" Recall: {recall:.4f}\")\n",
        "print(f\" F1 Score: {f1:.4f}\")\n",
        "print(f\" AUC-ROC: {auc_roc:.4f}\")\n",
        "\n",
        "# Plotting AUC-ROC Curve\n",
        "fpr, tpr, _ = roc_curve(actual_labels, predictions)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"AUC = {auc_roc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"AUC-ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g9icMNmvktpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion on the first Batch Inference Job with the Fine-Tune XGboost"
      ],
      "metadata": {
        "id": "QLqZdljKkyTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy (73.56%)\n",
        "\n",
        "    - The model is correctly predicting about 73.56% of the instances.\n",
        "\n",
        "- Precision (82.50%)\n",
        "\n",
        "    - When the model predicts positive (1), it is correct 82.50% of the time.\n",
        "    - High precision suggests fewer false positives.\n",
        "\n",
        "- Recall (83.76%)\n",
        "\n",
        "    - The model captures 83.76% of the actual positive cases.\n",
        "    - This indicates that it is detecting most of the positive instances.\n",
        "\n",
        "- F1 Score (83.12%)\n",
        "\n",
        "    - This is the harmonic mean of precision and recall.\n",
        "    - A high F1-score means a good balance between false positives and false negatives.\n",
        "\n",
        "- AUC-ROC Score (0.6831)\n",
        "\n",
        "    - This measures the model’s ability to distinguish between positive and negative classes.\n",
        "    - 0.6831 is low, meaning there’s room for improvement in separating the two classes."
      ],
      "metadata": {
        "id": "qyAMSmQDkzl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling Model Bias Monitor\n"
      ],
      "metadata": {
        "id": "rrBEfP0Xk3or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "\n",
        "ray.init(object_store_memory=2 * 1024 * 1024 * 1024)  # 2GB memory"
      ],
      "metadata": {
        "id": "AcvKWCqOK27B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install awswrangler"
      ],
      "metadata": {
        "id": "OVNIeuVMK7Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import sagemaker\n",
        "import boto3\n",
        "import joblib\n",
        "import tarfile\n",
        "from sagemaker import image_uris, get_execution_role, session\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "jPnHKw1LK81h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import awswrangler as wr\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Define S3 Paths\n",
        "s3_bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_prefix = \"Project\"\n",
        "\n",
        "train_path = f\"s3://{s3_bucket}/{s3_prefix}/train_data_full.csv\"\n",
        "validation_path = f\"s3://{s3_bucket}/{s3_prefix}/validation_data_full.csv\"\n",
        "test_path = f\"s3://{s3_bucket}/{s3_prefix}/test_data_full.csv\"\n",
        "production_path = f\"s3://{s3_bucket}/{s3_prefix}/production_data_full.csv\"\n",
        "\n",
        "# Load Datasets\n",
        "df_train = wr.s3.read_csv(train_path)\n",
        "df_validation = wr.s3.read_csv(validation_path)\n",
        "df_test = wr.s3.read_csv(test_path)\n",
        "df_production = wr.s3.read_csv(production_path)\n",
        "\n",
        "print(\" Loaded Original Datasets Successfully!\")\n",
        "\n",
        "# Extract unique department mappings dynamically\n",
        "df_departments = pd.concat([\n",
        "    df_train[[\"department_id\", \"aisle\"]],\n",
        "    df_validation[[\"department_id\", \"aisle\"]],\n",
        "    df_test[[\"department_id\", \"aisle\"]],\n",
        "    df_production[[\"department_id\", \"aisle\"]]\n",
        "]).drop_duplicates().rename(columns={\"aisle\": \"department_category\"})\n",
        "\n",
        "# Ensure unique mapping\n",
        "df_departments.drop_duplicates(inplace=True)\n",
        "\n",
        "# Save extracted department mapping to S3 (for reference)\n",
        "department_mapping_path = f\"s3://{s3_bucket}/{s3_prefix}/department_mapping_extracted.csv\"\n",
        "wr.s3.to_csv(df_departments, department_mapping_path, index=False)\n",
        "print(f\" Extracted Department Mapping Saved at: {department_mapping_path}\")\n",
        "\n",
        "# **Step 1: Apply Label Encoding for Aisle & Department**\n",
        "label_encoder_aisle = LabelEncoder()\n",
        "label_encoder_department = LabelEncoder()\n",
        "\n",
        "# Fit encoders on the department categories and aisles\n",
        "all_aisle_ids = df_departments[\"department_category\"]\n",
        "all_department_ids = df_departments[\"department_id\"]\n",
        "\n",
        "label_encoder_aisle.fit(all_aisle_ids)\n",
        "label_encoder_department.fit(all_department_ids)\n",
        "\n",
        "# **Step 2: Define Standard Scaler**\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# **Step 3: Processing Function for Each Dataset**\n",
        "def preprocess_dataset(df, dataset_name):\n",
        "    df = df.copy()\n",
        "\n",
        "    # **Apply Label Encoding**\n",
        "    df[\"aisle_id\"] = label_encoder_aisle.transform(df[\"aisle\"])\n",
        "    df[\"department_id_encoded\"] = label_encoder_department.transform(df[\"department_id\"])\n",
        "\n",
        "    # **Keep the original department category**\n",
        "    df[\"department_category\"] = df[\"aisle\"]  # Ensure category is retained for Bias Monitor\n",
        "\n",
        "    # **Apply Standard Scaling (ONLY to numerical features)**\n",
        "    numeric_features = [\"user_id\", \"product_id\", \"aisle_id\", \"department_id_encoded\"]\n",
        "    scaled_values = scaler.fit_transform(df[numeric_features])\n",
        "\n",
        "    # Convert to DataFrame with proper column names\n",
        "    df_scaled = pd.DataFrame(scaled_values, columns=numeric_features)\n",
        "\n",
        "    # **Restore \"reordered\" column**\n",
        "    df_scaled[\"reordered\"] = df[\"reordered\"].values\n",
        "    df_scaled.insert(0, \"reordered\", df_scaled.pop(\"reordered\"))  # Move to first column\n",
        "\n",
        "    # **Add department_category for Bias Monitoring**\n",
        "    df_scaled[\"department_category\"] = df[\"department_category\"]\n",
        "\n",
        "    # **Save the Transformed Dataset**\n",
        "    transformed_path = f\"s3://{s3_bucket}/{s3_prefix}/{dataset_name}_transformed_mapped.csv\"\n",
        "    wr.s3.to_csv(df_scaled, transformed_path, index=False)\n",
        "\n",
        "    print(f\" Transformed {dataset_name} Data Saved at: {transformed_path}\")\n",
        "    return df_scaled\n",
        "\n",
        "# **Step 4: Process All Datasets**\n",
        "df_train_scaled = preprocess_dataset(df_train, \"train_data\")\n",
        "df_validation_scaled = preprocess_dataset(df_validation, \"validation_data\")\n",
        "df_test_scaled = preprocess_dataset(df_test, \"test_data\")\n",
        "df_production_scaled = preprocess_dataset(df_production, \"production_data\")\n",
        "\n",
        "print(\"\\n **Preprocessing Pipeline Completed Successfully! Data is Fully Ready for Model Training & Bias Monitoring.**\")"
      ],
      "metadata": {
        "id": "app_6kavK-99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import awswrangler as wr\n",
        "import pandas as pd\n",
        "\n",
        "# Define S3 Paths\n",
        "s3_bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_prefix = \"Project\"\n",
        "\n",
        "# Load Transformed Datasets\n",
        "train_transformed_path = f\"s3://{s3_bucket}/{s3_prefix}/train_data_transformed_mapped.csv\"\n",
        "validation_transformed_path = f\"s3://{s3_bucket}/{s3_prefix}/validation_data_transformed_mapped.csv\"\n",
        "test_transformed_path = f\"s3://{s3_bucket}/{s3_prefix}/test_data_transformed_mapped.csv\"\n",
        "production_transformed_path = f\"s3://{s3_bucket}/{s3_prefix}/production_data_transformed_mapped.csv\"\n",
        "\n",
        "df_train_transformed = wr.s3.read_csv(train_transformed_path)\n",
        "df_validation_transformed = wr.s3.read_csv(validation_transformed_path)\n",
        "df_test_transformed = wr.s3.read_csv(test_transformed_path)\n",
        "df_production_transformed = wr.s3.read_csv(production_transformed_path)\n",
        "\n",
        "print(\" Successfully loaded all transformed datasets!\")\n",
        "\n",
        "# **Check for Missing Values**\n",
        "for name, df in zip([\"Train\", \"Validation\", \"Test\", \"Production\"],\n",
        "                     [df_train_transformed, df_validation_transformed, df_test_transformed, df_production_transformed]):\n",
        "    missing_values = df.isnull().sum().sum()\n",
        "    print(f\" {name} Data - Missing Values: {missing_values}\")\n",
        "\n",
        "# **Check Unique Department Categories in Each Dataset**\n",
        "for name, df in zip([\"Train\", \"Validation\", \"Test\", \"Production\"],\n",
        "                     [df_train_transformed, df_validation_transformed, df_test_transformed, df_production_transformed]):\n",
        "    unique_departments = df[\"department_category\"].unique()\n",
        "    print(f\" {name} Data - Unique Department Categories: {unique_departments}\")\n",
        "\n",
        "# **Check Standardized Columns**\n",
        "expected_columns = [\"reordered\", \"user_id\", \"product_id\", \"aisle_id\", \"department_id_encoded\", \"department_category\"]\n",
        "for name, df in zip([\"Train\", \"Validation\", \"Test\", \"Production\"],\n",
        "                     [df_train_transformed, df_validation_transformed, df_test_transformed, df_production_transformed]):\n",
        "    if all(col in df.columns for col in expected_columns):\n",
        "        print(f\" {name} Data - All expected columns are present.\")\n",
        "    else:\n",
        "        print(f\" {name} Data - Missing expected columns!\")\n",
        "\n",
        "print(\"\\n **Verification Completed!** If all checks pass, the dataset is ready for training.\")"
      ],
      "metadata": {
        "id": "kUH3VIHzLCnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import json\n",
        "import awswrangler as wr\n",
        "import io\n",
        "from sagemaker.estimator import Estimator\n",
        "from sagemaker.inputs import TrainingInput\n",
        "from time import gmtime, strftime\n",
        "\n",
        "# Initializing SageMaker session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Define S3 bucket and prefix\n",
        "bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "prefix = \"Project\"\n",
        "\n",
        "# Corrected dataset paths\n",
        "train_s3_path = f\"s3://{bucket}/{prefix}/train_data_transformed_mapped.csv\"\n",
        "val_s3_path = f\"s3://{bucket}/{prefix}/validation_data_transformed_mapped.csv\"\n",
        "output_s3_path = f\"s3://{bucket}/{prefix}/models/\"\n",
        "\n",
        "# Load data from S3 to verify structure\n",
        "df_train = wr.s3.read_csv(train_s3_path)\n",
        "df_validation = wr.s3.read_csv(val_s3_path)\n",
        "\n",
        "# Ensure \"department_category\" exists\n",
        "if \"department_category\" not in df_train.columns:\n",
        "    raise ValueError(\" 'department_category' column is missing from training data!\")\n",
        "\n",
        "# Remove department_category from training data\n",
        "train_columns = [col for col in df_train.columns if col != \"department_category\"]\n",
        "df_train_filtered = df_train[train_columns]\n",
        "df_validation_filtered = df_validation[train_columns]\n",
        "\n",
        "# Upload the cleaned datasets\n",
        "train_filtered_s3_path = f\"s3://{bucket}/{prefix}/train_data_final.csv\"\n",
        "val_filtered_s3_path = f\"s3://{bucket}/{prefix}/validation_data_final.csv\"\n",
        "\n",
        "wr.s3.to_csv(df_train_filtered, train_filtered_s3_path, index=False)\n",
        "wr.s3.to_csv(df_validation_filtered, val_filtered_s3_path, index=False)\n",
        "\n",
        "# Save column names separately for later reference\n",
        "column_names = df_train.columns.tolist()\n",
        "column_names_path = f\"s3://{bucket}/{prefix}/column_names.json\"\n",
        "\n",
        "# Convert JSON data to bytes\n",
        "json_bytes = json.dumps(column_names).encode(\"utf-8\")\n",
        "\n",
        "# Upload JSON file to S3 using awswrangler\n",
        "wr.s3.upload(\n",
        "    local_file=io.BytesIO(json_bytes),  # Use BytesIO to handle binary data\n",
        "    path=column_names_path\n",
        ")\n",
        "\n",
        "print(f\" Cleaned train & validation datasets saved at: {train_filtered_s3_path}, {val_filtered_s3_path}\")\n",
        "print(f\" Column names JSON saved to: {column_names_path}\")\n",
        "\n",
        "# Get SageMaker execution role\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "# Define unique job name\n",
        "job_name = \"xgb-reorder-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
        "\n",
        "# Retrieve XGBoost container image\n",
        "image_uri = sagemaker.image_uris.retrieve(\n",
        "    \"xgboost\", boto3.Session().region_name, version=\"1.5-1\"\n",
        ")\n",
        "\n",
        "# Define XGBoost Estimator\n",
        "xgb_estimator = Estimator(\n",
        "    image_uri=image_uri,\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    volume_size=50,\n",
        "    output_path=output_s3_path,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "# Set XGBoost hyperparameters\n",
        "xgb_estimator.set_hyperparameters(\n",
        "    objective=\"binary:logistic\",  # Required for classification\n",
        "    num_round=200,                # Equivalent to n_estimators\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    scale_pos_weight=0.4\n",
        ")\n",
        "\n",
        "# Define Training & Validation Data Inputs\n",
        "train_data = TrainingInput(train_filtered_s3_path, content_type=\"text/csv\")\n",
        "validation_data = TrainingInput(val_filtered_s3_path, content_type=\"text/csv\")\n",
        "\n",
        "# Start Training\n",
        "xgb_estimator.fit({\"train\": train_data, \"validation\": validation_data}, job_name=job_name)\n",
        "\n",
        "print(f\" Training Completed: {job_name}\")"
      ],
      "metadata": {
        "id": "wHweBlaiLJV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "from sagemaker.model import Model\n",
        "from sagemaker import get_execution_role\n",
        "from time import gmtime, strftime\n",
        "\n",
        "# Initialize SageMaker session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Define S3 bucket and model artifact path\n",
        "bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "prefix = \"Project\"\n",
        "model_artifact_path = f\"s3://{bucket}/{prefix}/models/xgb-reorder-2025-02-19-03-14-27/output/model.tar.gz\"\n",
        "\n",
        "# Define Model Registry Group Name\n",
        "model_package_group_name = \"XGBoost-Reorder-Predictions\"\n",
        "\n",
        "# Get SageMaker execution role\n",
        "role = get_execution_role()\n",
        "\n",
        "# Retrieve XGBoost Image URI\n",
        "image_uri = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, version=\"1.5-1\")\n",
        "\n",
        "#  Ensure the Model Package Group Exists\n",
        "sagemaker_client = boto3.client(\"sagemaker\")\n",
        "\n",
        "try:\n",
        "    # Check if the model package group already exists\n",
        "    sagemaker_client.describe_model_package_group(ModelPackageGroupName=model_package_group_name)\n",
        "    print(f\" Model Package Group '{model_package_group_name}' already exists.\")\n",
        "except sagemaker_client.exceptions.ClientError:\n",
        "    # Create the model package group if it doesn't exist\n",
        "    sagemaker_client.create_model_package_group(\n",
        "        ModelPackageGroupName=model_package_group_name,\n",
        "        ModelPackageGroupDescription=\"Model package group for XGBoost reorder predictions\",\n",
        "    )\n",
        "    print(f\" Created Model Package Group: {model_package_group_name}\")\n",
        "\n",
        "# Define Model Object\n",
        "xgb_model = Model(\n",
        "    model_data=model_artifact_path,\n",
        "    role=role,\n",
        "    image_uri=image_uri,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "# Register Model in SageMaker Model Registry\n",
        "model_package_response = sagemaker_session.sagemaker_client.create_model_package(\n",
        "    ModelPackageGroupName=model_package_group_name,\n",
        "    ModelPackageDescription=\"XGBoost model for reorder prediction\",\n",
        "    InferenceSpecification={\n",
        "        \"Containers\": [\n",
        "            {\n",
        "                \"Image\": image_uri,\n",
        "                \"ModelDataUrl\": model_artifact_path,\n",
        "                \"Environment\": {},\n",
        "            }\n",
        "        ],\n",
        "        \"SupportedContentTypes\": [\"text/csv\"],\n",
        "        \"SupportedResponseMIMETypes\": [\"text/csv\"],\n",
        "    },\n",
        "    ModelApprovalStatus=\"PendingManualApproval\",  # Change to \"Approved\" if needed\n",
        ")\n",
        "\n",
        "print(f\" Model Registered Successfully in SageMaker Model Registry!\")\n",
        "print(f\" Model Package ARN: {model_package_response['ModelPackageArn']}\")\n"
      ],
      "metadata": {
        "id": "T4Y_vgGCLN2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import os\n",
        "import tarfile\n",
        "import xgboost as xgb\n",
        "\n",
        "#  Define S3 bucket and model path\n",
        "bucket = \"sagemaker-us-east-1-921916832724\"  # Your S3 bucket\n",
        "model_key = \"Project/models/xgb-reorder-2025-02-19-03-14-27/output/model.tar.gz\"\n",
        "local_model_path = \"model.tar.gz\"\n",
        "extract_folder = \"xgb_model\"\n",
        "\n",
        "#  Download model from S3\n",
        "print(f\" Downloading model from: s3://{bucket}/{model_key}\")\n",
        "s3 = boto3.client(\"s3\")\n",
        "s3.download_file(bucket, model_key, local_model_path)\n",
        "\n",
        "#  Extract model\n",
        "print(f\" Extracting model to: {extract_folder}\")\n",
        "with tarfile.open(local_model_path, \"r:gz\") as tar:\n",
        "    tar.extractall(extract_folder)\n",
        "\n",
        "#  Find the extracted model file\n",
        "model_file_path = os.path.join(extract_folder, \"xgboost-model\")  # Correct path\n",
        "\n",
        "if not os.path.exists(model_file_path):\n",
        "    raise FileNotFoundError(f\" Model file not found at: {model_file_path}\")\n",
        "\n",
        "#  Load the XGBoost model\n",
        "print(f\" Loading model from: {model_file_path}\")\n",
        "xgb_model = xgb.Booster()\n",
        "xgb_model.load_model(model_file_path)\n",
        "\n",
        "print(\" Model successfully loaded and ready for inference!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "aoFN35LULQFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "#  Initialize SageMaker client\n",
        "sagemaker_client = boto3.client(\"sagemaker\")\n",
        "\n",
        "#  Define Model Package Group Name\n",
        "model_package_group_name = \"XGBoost-Reorder-Predictions\"\n",
        "\n",
        "# List model packages in the registry\n",
        "response = sagemaker_client.list_model_packages(\n",
        "    ModelPackageGroupName=model_package_group_name,\n",
        "    MaxResults=5,  #  Fetch last 5 model versions\n",
        "    SortBy=\"CreationTime\",\n",
        "    SortOrder=\"Descending\"\n",
        ")\n",
        "\n",
        "#  Display Model Package Versions\n",
        "if \"ModelPackageSummaryList\" in response:\n",
        "    print(f\"\\n Found Model Package Group: {model_package_group_name}\\n\")\n",
        "    for model in response[\"ModelPackageSummaryList\"]:\n",
        "        print(f\" Model Package Version: {model['ModelPackageVersion']}\")\n",
        "        print(f\"   Model Package ARN: {model['ModelPackageArn']}\")\n",
        "        print(f\"   Approval Status: {model['ModelApprovalStatus']}\")\n",
        "        print(f\"   Created On: {model['CreationTime']}\\n\")\n",
        "else:\n",
        "    print(\" No model package found in SageMaker Model Registry!\")"
      ],
      "metadata": {
        "id": "wtvne0xfLQ4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "#  Initialize SageMaker client\n",
        "sagemaker_client = boto3.client(\"sagemaker\")\n",
        "\n",
        "#  Define Model Package Group Name\n",
        "model_package_group_name = \"XGBoost-Reorder-Predictions\"\n",
        "\n",
        "# List model packages in the registry\n",
        "response = sagemaker_client.list_model_packages(\n",
        "    ModelPackageGroupName=model_package_group_name,\n",
        "    MaxResults=5,  #  Fetch last 5 model versions\n",
        "    SortBy=\"CreationTime\",\n",
        "    SortOrder=\"Descending\"\n",
        ")\n",
        "\n",
        "#  Display Model Package Versions\n",
        "if \"ModelPackageSummaryList\" in response:\n",
        "    print(f\"\\n Found Model Package Group: {model_package_group_name}\\n\")\n",
        "    for model in response[\"ModelPackageSummaryList\"]:\n",
        "        print(f\" Model Package Version: {model['ModelPackageVersion']}\")\n",
        "        print(f\"   Model Package ARN: {model['ModelPackageArn']}\")\n",
        "        print(f\"   Approval Status: {model['ModelApprovalStatus']}\")\n",
        "        print(f\"   Created On: {model['CreationTime']}\\n\")\n",
        "else:\n",
        "    print(\" No model package found in SageMaker Model Registry!\")"
      ],
      "metadata": {
        "id": "7zzK_W0ILUyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Approving the model package\n",
        "sagemaker_client.update_model_package(\n",
        "    ModelPackageArn=\"arn:aws:sagemaker:us-east-1:921916832724:model-package/XGBoost-Reorder-Predictions/1\",\n",
        "    ModelApprovalStatus=\"Approved\"\n",
        ")\n",
        "\n",
        "print(\" Model Approved!\")"
      ],
      "metadata": {
        "id": "D-FLnP9TLVk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.model import Model\n",
        "\n",
        "#  Define Model Package ARN (Ensure you copy the correct one)\n",
        "model_package_arn = \"arn:aws:sagemaker:us-east-1:921916832724:model-package/XGBoost-Reorder-Predictions/1\"\n",
        "\n",
        "#  SageMaker Session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "#  Create SageMaker Model from the Model Package\n",
        "xgb_model = Model(\n",
        "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, version=\"1.5-1\"),\n",
        "    model_data=f\"s3://sagemaker-us-east-1-921916832724/Project/models/xgb-reorder-2025-02-19-03-14-27/output/model.tar.gz\",\n",
        "    role=role,\n",
        "    sagemaker_session=sagemaker_session,\n",
        "    name=\"XGBoost-Reorder-Predictions\"\n",
        ")\n",
        "\n",
        "#  Deploy as a SageMaker Model (but NOT an endpoint)\n",
        "xgb_model.create()\n",
        "print(\" Model Created Successfully in SageMaker!\")"
      ],
      "metadata": {
        "id": "IlftKQMaLXj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Load the test dataset\n",
        "test_data_path = \"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed_mapped.csv\"  # Update if needed\n",
        "df_test = pd.read_csv(test_data_path)\n",
        "\n",
        "# Keep `department_category` for later analysis\n",
        "df_test_original = df_test.copy()  # Store original data with department_category\n",
        "department_category = df_test_original[\"department_category\"]  # Extract the column\n",
        "\n",
        "# Separate features and labels\n",
        "X_test = df_test.drop(columns=[\"reordered\", \"department_category\"])  # Remove target & category\n",
        "y_test = df_test[\"reordered\"]\n",
        "\n",
        "# Convert to DMatrix format (required for XGBoost)\n",
        "dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_probs = xgb_model.predict(dtest)\n",
        "y_pred = np.where(y_pred_probs > 0.5, 1, 0)  # Convert probabilities to binary class (Threshold = 0.5)\n",
        "\n",
        "# Add Predictions Back to Original Data\n",
        "df_test_original[\"predicted_label\"] = y_pred\n",
        "df_test_original[\"predicted_probability\"] = y_pred_probs\n",
        "\n",
        "# Evaluate Performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc_roc = roc_auc_score(y_test, y_pred_probs)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(f\" Model Evaluation Results:\")\n",
        "print(f\" Accuracy: {accuracy:.4f}\")\n",
        "print(f\" Precision: {precision:.4f}\")\n",
        "print(f\" Recall: {recall:.4f}\")\n",
        "print(f\" F1-score: {f1:.4f}\")\n",
        "print(f\" AUC-ROC: {auc_roc:.4f}\")\n",
        "print(\" Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "#  AUC-ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"AUC-ROC = {auc_roc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#  Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not Reordered\", \"Reordered\"], yticklabels=[\"Not Reordered\", \"Reordered\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b1uNOCBrLbkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import boto3\n",
        "\n",
        "# S3 bucket details\n",
        "bucket_name = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_key = \"Project/test_data_transformed_mapped.csv\"\n",
        "local_file_path = \"test_data_transformed_mapped_no_headers.csv\"\n",
        "\n",
        "#  Read CSV from S3 using s3fs\n",
        "df = pd.read_csv(f\"s3://{bucket_name}/{s3_key}\", storage_options={\"anon\": False})\n",
        "\n",
        "#  Drop the headers by saving without header row\n",
        "df.to_csv(local_file_path, index=False, header=False)\n",
        "\n",
        "# Upload the updated file back to S3\n",
        "s3_client = boto3.client(\"s3\")\n",
        "s3_client.upload_file(local_file_path, bucket_name, \"Project/test_data_transformed_mapped_no_headers.csv\")\n",
        "\n",
        "print(\"Updated file uploaded successfully to S3!\")"
      ],
      "metadata": {
        "id": "u27rKwUTLciR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  Define correct column names\n",
        "columns = [\"reordered\", \"user_id\", \"product_id\", \"aisle_id\", \"department_id_encoded\", \"department_category\"]\n",
        "\n",
        "#  Load CSV with proper columns\n",
        "file_path = \"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed_mapped.csv\"\n",
        "df = pd.read_csv(file_path, names=columns, header=None)\n",
        "\n",
        "#  Drop only the \"department_category\" column\n",
        "df = df.drop(columns=[\"department_category\"], errors=\"ignore\")\n",
        "\n",
        "#  Save without headers (target column included)\n",
        "cleaned_file_path = \"test_data_transformed_mapped_with_target.csv\"\n",
        "df.to_csv(cleaned_file_path, index=False, header=False)\n",
        "\n",
        "print(\" Dataset cleaned and saved with target column:\", cleaned_file_path)"
      ],
      "metadata": {
        "id": "rbc-QKYOLgeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "u_LPwtlULj97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  Load CSV **without headers**\n",
        "file_path = \"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed_mapped.csv\"\n",
        "df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "#  Drop only the \"department_category\" column (last column)\n",
        "df = df.iloc[:, :-1]  # Drop the last column\n",
        "\n",
        "#  Save the cleaned dataset without headers\n",
        "cleaned_file_path = \"test_data_transformed_mapped_no_headers.csv\"\n",
        "df.to_csv(cleaned_file_path, index=False, header=False)\n",
        "\n",
        "print(\" Dataset cleaned and saved without headers:\", cleaned_file_path)\n"
      ],
      "metadata": {
        "id": "YR_RuIo1LkpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "s3_client = boto3.client(\"s3\")\n",
        "s3_bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_key = \"Project/test_data_transformed_mapped_no_headers.csv\"\n",
        "\n",
        "s3_client.upload_file(cleaned_file_path, s3_bucket, s3_key)\n",
        "\n",
        "print(\" Updated dataset uploaded to S3:\", f\"s3://{s3_bucket}/{s3_key}\")"
      ],
      "metadata": {
        "id": "irJi_ADvLnxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.transformer import Transformer\n",
        "\n",
        "#  Path to the updated dataset in S3\n",
        "test_data_s3_path = \"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed_mapped_no_headers.csv\"\n",
        "\n",
        "#  Define Transformer\n",
        "transformer = Transformer(\n",
        "    model_name=\"XGBoost-Reorder-Predictions\",\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    strategy=\"MultiRecord\",\n",
        "    assemble_with=\"Line\",\n",
        "    output_path=\"s3://sagemaker-us-east-1-921916832724/Project/BatchInferenceResults/\",\n",
        "    accept=\"text/csv\"\n",
        ")\n",
        "\n",
        "#  Execute Batch Transform Job\n",
        "transformer.transform(\n",
        "    data=test_data_s3_path,\n",
        "    split_type=\"Line\",\n",
        "    content_type=\"text/csv\",\n",
        "    input_filter=\"$[1:]\",  #\n",
        "    join_source=\"Input\"\n",
        ")\n",
        "\n",
        "#  Wait for job completion\n",
        "transformer.wait()\n",
        "\n",
        "print(\" Batch inference job completed successfully!\")"
      ],
      "metadata": {
        "id": "WMgJ6VC3Lp7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sage Clarify Explainability Model"
      ],
      "metadata": {
        "id": "ckhlCTw5Lr7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "s3_client = boto3.client(\"s3\")\n",
        "s3_bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_prefix = \"Project/BatchInferenceResults/\"\n",
        "\n",
        "response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
        "\n",
        "print(\" Files in Batch Inference Output:\")\n",
        "for obj in response.get(\"Contents\", []):\n",
        "    print(obj[\"Key\"])"
      ],
      "metadata": {
        "id": "j93jYnAaLtkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"test_data_predictions.csv\"\n",
        "s3_client.download_file(s3_bucket, \"Project/BatchInferenceResults/test_data_transformed_mapped_no_headers.csv.out\", output_file)\n",
        "\n",
        "# Load the results\n",
        "import pandas as pd\n",
        "df_predictions = pd.read_csv(output_file, header=None)\n",
        "\n",
        "print(\" First 5 predictions:\")\n",
        "print(df_predictions.head())"
      ],
      "metadata": {
        "id": "2GyxJx27Lvy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import boto3\n",
        "\n",
        "#  **Define Expected Headers**\n",
        "headers = [\"reordered\", \"user_id\", \"product_id\", \"aisle_id\", \"department_id_encoded\", \"probability\"]\n",
        "\n",
        "#  **S3 Paths**\n",
        "s3_bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_input_path = \"Project/BatchInferenceResults/test_data_transformed_mapped_no_headers.csv.out\"\n",
        "s3_output_path = \"Project/BatchInferenceResults/test_data_transformed_mapped_with_headers.csv\"\n",
        "\n",
        "#  **Download the File from S3**\n",
        "s3_client = boto3.client(\"s3\")\n",
        "local_file = \"test_data_transformed_mapped_no_headers.csv.out\"\n",
        "updated_file = \"test_data_transformed_mapped_with_headers.csv\"\n",
        "\n",
        "s3_client.download_file(s3_bucket, s3_input_path, local_file)\n",
        "\n",
        "#  **Load CSV Without Headers**\n",
        "df = pd.read_csv(local_file, header=None)\n",
        "\n",
        "#  **Assign Correct Headers**\n",
        "df.columns = headers\n",
        "\n",
        "#  **Save File with Headers**\n",
        "df.to_csv(updated_file, index=False, header=True)\n",
        "\n",
        "#  **Upload Back to S3**\n",
        "s3_client.upload_file(updated_file, s3_bucket, s3_output_path)\n",
        "\n",
        "print(f\" Updated File Uploaded: s3://{s3_bucket}/{s3_output_path}\")"
      ],
      "metadata": {
        "id": "GRppu3TDLy6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"test_data_transformed_mapped_with_headers.csv\")\n",
        "print(df[\"department_id_encoded\"].unique())"
      ],
      "metadata": {
        "id": "Pv5rLPRcL045"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset (Replace with actual S3 path)\n",
        "df = pd.read_csv(\"s3://sagemaker-us-east-1-921916832724/Project/BatchInferenceResults/test_data_transformed_mapped_fixed.csv\")\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "tSwaMAYtL1jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Libraries\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker import Session\n",
        "from sagemaker.s3 import S3Downloader, S3Uploader\n",
        "from sagemaker.clarify import DataConfig, ModelConfig, SHAPConfig, SageMakerClarifyProcessor\n",
        "\n",
        "# Initialize SageMaker Session\n",
        "session = Session()\n",
        "role = sagemaker.get_execution_role()\n",
        "region = session.boto_region_name\n",
        "\n",
        "# Define S3 Bucket and Prefix for the Project\n",
        "bucket = \"sagemaker-us-east-1-921916832724\"\n",
        "prefix = \"Project\"\n",
        "\n",
        "# Define S3 Paths for Explainability\n",
        "s3_data_path = f\"s3://{bucket}/{prefix}/BatchInferenceResults/test_data_transformed_mapped_fixed.csv\"\n",
        "s3_output_path = f\"s3://{bucket}/{prefix}/explainability_output\"\n",
        "\n",
        "# Define Data Config for Clarify\n",
        "data_config = DataConfig(\n",
        "    s3_data_input_path=s3_data_path,  # Use the correct dataset without headers\n",
        "    s3_output_path=s3_output_path,\n",
        "    label=\"reordered\",  # Target column\n",
        "    headers=[\"reordered\", \"user_id\", \"product_id\", \"aisle_id\", \"department_id_encoded\"],\n",
        "    dataset_type=\"text/csv\"\n",
        ")\n",
        "\n",
        "print(\" Data Config Set!\")\n",
        "\n",
        "# Define Model Config\n",
        "model_config = ModelConfig(\n",
        "    model_name=\"XGBoost-Reorder-Predictions\",  # Trained XGBoost Model in SageMaker\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    instance_count=1,\n",
        "    content_type=\"text/csv\",\n",
        "    accept_type=\"text/csv\"\n",
        ")\n",
        "\n",
        "print(\" Model Config Set!\")\n",
        "\n",
        "# Define SHAP (Explainability) Config\n",
        "shap_config = SHAPConfig(\n",
        "    baseline=[[-1, -1, -1, -1]],  # Dummy baseline (adjust as needed)\n",
        "    num_samples=50,  # Number of samples to use for SHAP\n",
        "    agg_method=\"mean_abs\",\n",
        "    save_local_shap_values=True  # Save detailed SHAP values\n",
        ")\n",
        "\n",
        "print(\" SHAP Config Set!\")\n",
        "\n",
        "# Initialize SageMaker Clarify Processor\n",
        "clarify_processor = SageMakerClarifyProcessor(\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    sagemaker_session=session\n",
        ")\n",
        "\n",
        "print(\" Clarify Processor Initialized!\")\n",
        "\n",
        "# Run Explainability Job\n",
        "clarify_processor.run_explainability(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    explainability_config=shap_config\n",
        ")\n",
        "\n",
        "print(f\" Explainability Job Started! Results will be saved to: {s3_output_path}\")"
      ],
      "metadata": {
        "id": "jIRazM1PL3SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "qeZGwNrVL8DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Define S3 parameters\n",
        "bucket_name = \"sagemaker-us-east-1-921916832724\"\n",
        "s3_prefix = \"Project/explainability_output\"\n",
        "s3_file_name = \"explanations_shap/out.csv\"  # SHAP output file\n",
        "\n",
        "# Local path to save the SHAP file\n",
        "local_shap_file = \"/tmp/shap_out.csv\"\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client(\"s3\")\n",
        "\n",
        "# Download SHAP output from S3\n",
        "s3_client.download_file(bucket_name, f\"{s3_prefix}/{s3_file_name}\", local_shap_file)\n",
        "print(f\" SHAP output file downloaded: {local_shap_file}\")\n",
        "\n",
        "# Load SHAP values into Pandas DataFrame\n",
        "shap_df = pd.read_csv(local_shap_file)\n",
        "\n",
        "# Display the first few rows\n",
        "print(\" First few rows of SHAP explanations:\")\n",
        "print(shap_df.head())\n",
        "\n",
        "# -------------------------\n",
        "#  GLOBAL FEATURE IMPORTANCE\n",
        "# -------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap_values_mean = shap_df.abs().mean().sort_values(ascending=False)\n",
        "sns.barplot(x=shap_values_mean.values, y=shap_values_mean.index)\n",
        "plt.xlabel(\"Mean |SHAP Value|\", fontsize=12)\n",
        "plt.ylabel(\"Feature\", fontsize=12)\n",
        "plt.title(\"Global Feature Importance\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "#  SHAP SUMMARY PLOT (Without Explainer)\n",
        "# -------------------------\n",
        "shap_values_np = shap_df.values\n",
        "\n",
        "shap.summary_plot(shap_values_np, feature_names=shap_df.columns)\n"
      ],
      "metadata": {
        "id": "jkaWkKpJL8qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Global Feature Importance (First Image)**\n",
        "\n",
        "    - The product_id has the highest importance, meaning it has the most influence on whether a product is reordered.\n",
        "    - Department ID and Aisle ID also play significant roles.\n",
        "    - User ID has the least importance in the prediction.\n",
        "\n",
        "- **SHAP Summary Plot (Second Image)**\n",
        "\n",
        "    - Each dot represents a SHAP value for an instance in the dataset.\n",
        "    - A positive SHAP value means the feature increases the prediction probability.\n",
        "    - A negative SHAP value decreases the probability of a product being reordered.\n",
        "    - The spread of dots indicates how much variability a feature has in affecting the model's prediction."
      ],
      "metadata": {
        "id": "8ct6_9y1MBgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Zoom in on a specific feature (product_id)"
      ],
      "metadata": {
        "id": "U6HCPT7hMEGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ],
      "metadata": {
        "id": "eKEBFTZbL_1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load SHAP values\n",
        "shap_values = pd.read_csv(\"s3://sagemaker-us-east-1-921916832724/Project/explainability_output/explanations_shap/out.csv\")  # Update with your actual path\n",
        "features = shap_values.columns[:-1]  # Excluding label\n",
        "\n",
        "# Load the original dataset (if needed)\n",
        "data = pd.read_csv(\"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed_mapped.csv\")  # Update with actual path"
      ],
      "metadata": {
        "id": "Sy9BcSIcMIaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "id": "dLkRnF5DMLIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shap_values.columns)"
      ],
      "metadata": {
        "id": "1Oi-GbCCMMz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip \"_label0\" suffix from SHAP feature names\n",
        "shap_values.columns = [col.replace(\"_label0\", \"\") for col in shap_values.columns]"
      ],
      "metadata": {
        "id": "GUlBmmA7MNbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shap_values.columns)"
      ],
      "metadata": {
        "id": "huO-dBmAMPQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of shap_values:\", shap_values.shape)  # Should be (rows, 4)\n",
        "print(\"Shape of data:\", data.shape)  # Should be (rows, 4)"
      ],
      "metadata": {
        "id": "Ami7X5OFMTyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the extra column 'reordered' from data\n",
        "data_filtered = data.drop(columns=[\"reordered\"])\n",
        "\n",
        "# Print shapes to confirm they match\n",
        "print(\"Shape of shap_values:\", shap_values.shape)  # Should be (758816, 4)\n",
        "print(\"Shape of data_filtered:\", data_filtered.shape)  # Should be (758816, 4)\n",
        "\n",
        "# Ensure column names match\n",
        "print(\"SHAP Columns:\", shap_values.columns)\n",
        "print(\"Data Columns:\", data_filtered.columns)"
      ],
      "metadata": {
        "id": "vm3UNoNaMVgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Compute mean absolute SHAP values per feature\n",
        "shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "# Find the most influential product_id based on its SHAP value\n",
        "most_influential_product_index = shap_importance.argmax()\n",
        "\n",
        "# Print the most influential product ID\n",
        "most_influential_product_id = data_filtered[\"product_id\"].iloc[most_influential_product_index]\n",
        "print(\"Most influential product ID:\", most_influential_product_id)"
      ],
      "metadata": {
        "id": "sX7DciJCMXbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert department_category to categorical values for color mapping\n",
        "data_filtered['department_category'] = data_filtered['department_category'].astype('category')\n",
        "\n",
        "# Create color mapping for categories\n",
        "category_colors = dict(zip(data_filtered['department_category'].cat.categories, range(len(data_filtered['department_category'].cat.categories))))\n",
        "\n",
        "# Map the department_category column to numerical values for coloring\n",
        "color_values = data_filtered['department_category'].map(category_colors)\n",
        "\n",
        "# Generate the SHAP summary plot\n",
        "shap.summary_plot(shap_values.values, data_filtered[shap_values.columns], plot_type=\"dot\", color=color_values)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T6r0mg-jMbWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute mean absolute SHAP values grouped by department_category\n",
        "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.columns)\n",
        "shap_df['department_category'] = data_filtered['department_category']\n",
        "\n",
        "# Compute mean SHAP values for each category\n",
        "mean_shap_per_category = shap_df.groupby('department_category').mean().abs().sum(axis=1)\n",
        "\n",
        "# Identify the most influential department_category\n",
        "most_influential_category = mean_shap_per_category.idxmax()\n",
        "print(f\"Most Influential Department Category: {most_influential_category}\")\n",
        "\n",
        "# Filter data for only the most influential category\n",
        "filtered_data = data_filtered[data_filtered['department_category'] == most_influential_category]\n",
        "filtered_shap_values = shap_values.values[data_filtered['department_category'] == most_influential_category]\n",
        "\n",
        "# Generate SHAP summary plot for the most influential category\n",
        "shap.summary_plot(filtered_shap_values, filtered_data[shap_values.columns])\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ySFtSegMeTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute mean absolute SHAP values grouped by department_category\n",
        "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.columns)\n",
        "shap_df['department_category'] = data_filtered['department_category']\n",
        "\n",
        "# Compute mean SHAP values for each category\n",
        "mean_shap_per_category = shap_df.groupby('department_category').mean().abs().sum(axis=1)\n",
        "\n",
        "# Get the top 10 most influential categories\n",
        "top_10_categories = mean_shap_per_category.sort_values(ascending=False).head(10)\n",
        "\n",
        "# Print the top 10 influential categories\n",
        "print(\"Top 10 Most Influential Department Categories:\\n\", top_10_categories)\n",
        "\n",
        "# Plot the top 10 categories\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_categories.plot(kind='barh', color='steelblue')\n",
        "plt.xlabel(\"Mean Absolute SHAP Value\")\n",
        "plt.ylabel(\"Department Category\")\n",
        "plt.title(\"Top 10 Most Influential Department Categories (SHAP Analysis)\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "93sJX_-wMhGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute mean absolute SHAP values grouped by department_category\n",
        "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.columns)\n",
        "shap_df['department_category'] = data_filtered['department_category']\n",
        "\n",
        "# Compute mean SHAP values for each category\n",
        "mean_shap_per_category = shap_df.groupby('department_category').mean().abs().sum(axis=1)\n",
        "\n",
        "# Get the 10 least influential categories\n",
        "bottom_10_categories = mean_shap_per_category.sort_values(ascending=True).head(10)\n",
        "\n",
        "# Print the 10 least influential categories\n",
        "print(\"10 Least Influential Department Categories:\\n\", bottom_10_categories)\n",
        "\n",
        "# Plot the bottom 10 categories\n",
        "plt.figure(figsize=(10, 6))\n",
        "bottom_10_categories.plot(kind='barh', color='lightcoral')\n",
        "plt.xlabel(\"Mean Absolute SHAP Value\")\n",
        "plt.ylabel(\"Department Category\")\n",
        "plt.title(\"10 Least Influential Department Categories (SHAP Analysis)\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "03tIRR_EMh1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"s3://sagemaker-us-east-1-921916832724/Project/test_data_transformed_mapped.csv\")"
      ],
      "metadata": {
        "id": "A-KUUnWPMlCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'reordered' exists in data before adding it\n",
        "if 'reordered' in data.columns:\n",
        "    data_filtered['reordered'] = data['reordered']\n",
        "    print(\"Successfully added 'reordered' column to data_filtered!\")\n",
        "else:\n",
        "    print(\"Error: 'reordered' column not found in data!\")"
      ],
      "metadata": {
        "id": "TtlLDpbPMn7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean absolute SHAP values grouped by department_category\n",
        "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.columns)\n",
        "shap_df['department_category'] = data_filtered['department_category']\n",
        "shap_df['reordered'] = data_filtered['reordered']  # Add reordered column\n",
        "\n",
        "# Compute mean SHAP values for each category where reordered = 1 (reorders)\n",
        "mean_shap_per_category_reordered = (\n",
        "    shap_df[shap_df['reordered'] == 1]  # Filter only reordered entries\n",
        "    .groupby('department_category')\n",
        "    .mean()\n",
        "    .abs()\n",
        "    .sum(axis=1)  # Sum SHAP values for all features\n",
        ")\n",
        "\n",
        "# Get the top 10 most influential categories leading to reorders\n",
        "top_10_reordered_categories = mean_shap_per_category_reordered.sort_values(ascending=False).head(10)\n",
        "\n",
        "# Print the 10 most influential department categories for reorders\n",
        "print(\"Top 10 Department Categories Influencing Reorders:\\n\", top_10_reordered_categories)\n",
        "\n",
        "# Plot the top 10 department categories\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_reordered_categories.plot(kind='barh', color='royalblue')\n",
        "plt.xlabel(\"Mean Absolute SHAP Value\")\n",
        "plt.ylabel(\"Department Category\")\n",
        "plt.title(\"Top 10 Department Categories Leading to Reorders (SHAP Analysis)\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSaLz6_PMrot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter dataset for products that WERE reordered\n",
        "reordered_data = data_filtered[data_filtered['reordered'] == 1]\n",
        "\n",
        "# Compute the SHAP influence for each product_id in reordered cases\n",
        "shap_reordered = shap_values.values[data_filtered['reordered'] == 1]\n",
        "\n",
        "# Create a DataFrame to hold SHAP values for each product\n",
        "shap_reordered_df = pd.DataFrame(shap_reordered, columns=shap_values.columns)\n",
        "shap_reordered_df['product_id'] = reordered_data['product_id'].values\n",
        "\n",
        "# Aggregate SHAP values for each product\n",
        "shap_product_reordered = shap_reordered_df.groupby('product_id').mean().abs().sum(axis=1)\n",
        "\n",
        "# Get the **10 most influential products** for reordering\n",
        "top_10_most_influential_products = shap_product_reordered.nlargest(10)\n",
        "\n",
        "# Display results\n",
        "print(\"Top 10 Most Influential Products for Reordering:\")\n",
        "print(top_10_most_influential_products)"
      ],
      "metadata": {
        "id": "X9TjOeGWMuJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the department category for each product in the most influential list\n",
        "most_influential_products_df = top_10_most_influential_products.reset_index()\n",
        "most_influential_products_df = most_influential_products_df.merge(\n",
        "    data_filtered[['product_id', 'department_category']],\n",
        "    on='product_id',\n",
        "    how='left'\n",
        ").drop_duplicates()\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(most_influential_products_df['department_category'], most_influential_products_df.iloc[:, 1], color='royalblue')\n",
        "\n",
        "plt.xlabel(\"Mean Absolute SHAP Value\")\n",
        "plt.ylabel(\"Department Category\")\n",
        "plt.title(\"Top  Most Influential Products for Reordering\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QySrCgcQMu_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter dataset for products that were NOT reordered\n",
        "not_reordered_data = data_filtered[data_filtered['reordered'] == 0]\n",
        "\n",
        "# Compute the SHAP influence for each product_id\n",
        "shap_not_reordered = shap_values.values[data_filtered['reordered'] == 0]\n",
        "\n",
        "# Create a DataFrame to hold SHAP values for each product\n",
        "shap_not_reordered_df = pd.DataFrame(shap_not_reordered, columns=shap_values.columns)\n",
        "shap_not_reordered_df['product_id'] = not_reordered_data['product_id'].values\n",
        "\n",
        "# Aggregate SHAP values for each product\n",
        "shap_product_not_reordered = shap_not_reordered_df.groupby('product_id').mean().abs().sum(axis=1)\n",
        "\n",
        "# Get the 10 products with the **lowest** SHAP impact (least influential for reordering)\n",
        "least_influential_products = shap_product_not_reordered.nsmallest(10)\n",
        "\n",
        "# Display the result\n",
        "print(\"Top 10 Products Least Likely to Be Not Reordered:\")\n",
        "print(least_influential_products)"
      ],
      "metadata": {
        "id": "h1cI2ljbMypf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the department category for each product in the least influential list\n",
        "least_influential_products_df = least_influential_products.reset_index()\n",
        "least_influential_products_df = least_influential_products_df.merge(\n",
        "    data_filtered[['product_id', 'department_category']],\n",
        "    on='product_id',\n",
        "    how='left'\n",
        ").drop_duplicates()\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(least_influential_products_df['department_category'], least_influential_products_df.iloc[:, 1], color='darkorange')\n",
        "\n",
        "plt.xlabel(\"Mean Absolute SHAP Value\")\n",
        "plt.ylabel(\"Department Category\")\n",
        "plt.title(\"Top 10 Least Influential Products for Not Being Reordered\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1NeFNc4EMzNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure 'reordered' is in the dataset\n",
        "if 'reordered' not in data_filtered.columns:\n",
        "    print(\"Warning: 'reordered' column not found in dataset.\")\n",
        "\n",
        "# Compute mean absolute SHAP values grouped by department_category\n",
        "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.columns)\n",
        "shap_df['department_category'] = data_filtered['department_category']\n",
        "shap_df['reordered'] = data_filtered['reordered']  # Add reordered column\n",
        "\n",
        "# Compute mean SHAP values for each category where reordered = 0 (not reorders)\n",
        "mean_shap_per_category_not_reordered = (\n",
        "    shap_df[shap_df['reordered'] == 0]  # Filter only non-reordered entries\n",
        "    .groupby('department_category')\n",
        "    .mean()\n",
        "    .abs()\n",
        "    .sum(axis=1)  # Sum SHAP values for all features\n",
        ")\n",
        "\n",
        "# Get the top 10 most influential categories leading to not reorders\n",
        "top_10_not_reordered_categories = mean_shap_per_category_not_reordered.sort_values(ascending=False).head(10)\n",
        "\n",
        "# Print the 10 most influential department categories for not reorders\n",
        "print(\"Top 10 Department Categories Influencing Not Reorders:\\n\", top_10_not_reordered_categories)\n",
        "\n",
        "# Plot the top 10 department categories\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_not_reordered_categories.plot(kind='barh', color='crimson')\n",
        "plt.xlabel(\"Mean Absolute SHAP Value\")\n",
        "plt.ylabel(\"Department Category\")\n",
        "plt.title(\"Top 10 Department Categories Leading to Not Reorders (SHAP Analysis)\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zJIBjZUpM3wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of SHAP Analysis on Reordering Behavior**:\n",
        "\n",
        "\n",
        "1. General Feature Importance\n",
        "    - Product ID has the highest impact on reorder predictions, followed by Department ID (Encoded), Aisle ID, and User ID.\n",
        "    - The most influential feature suggests that the specific products and their respective department associations play a significant role in predicting reordering behavior.\n",
        "\n",
        "2. Most Influential Department Categories\n",
        "    - The top department categories driving reordering behavior include:\n",
        "        - Spices & Seasonings\n",
        "        - Baking Supplies & Decor\n",
        "        - Baking Ingredients\n",
        "        - Condiments\n",
        "        - Ice Cream Toppings\n",
        "        - Preserved Dips & Spreads\n",
        "        - Salad Dressing Toppings\n",
        "        - Doughs, Gelatins & Bake Mixes\n",
        "        - Milk\n",
        "        - Pickled Goods & Olives\n",
        "\n",
        "These categories typically consist of frequently used ingredients and perishables, which explains their likelihood of being reordered.\n",
        "\n",
        "3. Least Influential Department Categories\n",
        "    - The department categories with the least impact on reorder likelihood include:\n",
        "        - Fruit & Vegetable Snacks\n",
        "        - Other Creams & Cheeses\n",
        "        - Frozen Juice\n",
        "        - Frozen Breakfast\n",
        "        - Packaged Cheese\n",
        "        - Soft Drinks\n",
        "        - Refrigerated Pudding & Desserts\n",
        "        - Frozen Meals\n",
        "        - Frozen Appetizers & Sides\n",
        "        - Juice Nectars\n",
        "These categories may not be reordered frequently due to longer shelf life or sporadic purchase habits.\n",
        "\n",
        "4. Most Influential Products for Reordering\n",
        "    - The top individual products contributing to reorder likelihood are mapped to the following department categories:\n",
        "\n",
        "        - Spices & Seasonings\n",
        "        - Baking Ingredients\n",
        "        - Baking Supplies & Decor\n",
        "This reinforces the trend that frequently used ingredients are strong predictors of repeat purchases.\n",
        "\n",
        "5. Least Influential Products for Reordering\n",
        "    - The least impactful products (not likely to be reordered) belong to:\n",
        "        - Tortillas & Flat Bread\n",
        "        - Crackers\n",
        "        - Coffee\n",
        "        - Fresh Vegetables\n",
        "        - Lunch Meat\n",
        "        - Cookies & Cakes\n",
        "These products might be purchased in bulk, have longer consumption cycles, or are purchased occasionally.\n",
        "\n",
        "**Conclusion**\n",
        "The analysis successfully identified which product categories and specific products strongly drive reordering behavior.\n",
        "The most frequently reordered items tend to be core ingredients and consumables, while packaged, frozen, and impulse-purchase items are less likely to be reordered.\n",
        "The insights from this analysis can help businesses optimize inventory management, personalized recommendations, and customer retention strategies."
      ],
      "metadata": {
        "id": "9JM6UOzEM6Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Integration and Continuous Deployment"
      ],
      "metadata": {
        "id": "a6216-SbM-9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A SageMaker Pipeline"
      ],
      "metadata": {
        "id": "rOa0ukZfNRZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sagemaker\n"
      ],
      "metadata": {
        "id": "FRRMsUduM4aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.workflow.pipeline_context import PipelineSession\n",
        "\n",
        "sagemaker_session = sagemaker.session.Session()\n",
        "region = sagemaker_session.boto_region_name\n",
        "role = sagemaker.get_execution_role()\n",
        "pipeline_session = PipelineSession()\n",
        "default_bucket = sagemaker_session.default_bucket()\n",
        "model_package_group_name = f\"InstaCartModelPackageGroupName\""
      ],
      "metadata": {
        "id": "qNQrcEdhNhcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = f\"sagemaker-example-files-prod-{region}\""
      ],
      "metadata": {
        "id": "Fx8KHRaeNjGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data"
      ],
      "metadata": {
        "id": "66d0p8CKNkqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = \"data/02_19_2025_train_data_transformed_mapped.csv\"\n",
        "\n",
        "s3 = boto3.resource(\"s3\")\n",
        "\n",
        "base_uri = f\"s3://{default_bucket}/instacart\"\n",
        "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
        "    local_path=local_path,\n",
        "    desired_s3_uri=base_uri,\n",
        ")\n",
        "print(input_data_uri)"
      ],
      "metadata": {
        "id": "Hxx6fCGvNmNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = \"data/02_19_2025_production_data_transformed_mapped.csv\"\n",
        "\n",
        "s3 = boto3.resource(\"s3\")\n",
        "\n",
        "base_uri = f\"s3://{default_bucket}/instacart\"\n",
        "batch_data_uri = sagemaker.s3.S3Uploader.upload(\n",
        "    local_path=local_path,\n",
        "    desired_s3_uri=base_uri,\n",
        ")\n",
        "print(batch_data_uri)"
      ],
      "metadata": {
        "id": "aPM-uZ1UNn2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Parameters to Parametrize Pipeline Execution"
      ],
      "metadata": {
        "id": "WF1S0SvINqxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.workflow.parameters import (\n",
        "    ParameterInteger,\n",
        "    ParameterString,\n",
        "    ParameterFloat,\n",
        ")\n",
        "\n",
        "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
        "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
        "model_approval_status = ParameterString(\n",
        "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
        ")\n",
        "input_data = ParameterString(\n",
        "    name=\"InputData\",\n",
        "    default_value=input_data_uri,\n",
        ")\n",
        "batch_data = ParameterString(\n",
        "    name=\"BatchData\",\n",
        "    default_value=batch_data_uri,\n",
        ")\n",
        "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=6.0)"
      ],
      "metadata": {
        "id": "zWYYvgT_NqeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Processing Step for Feature Engineering"
      ],
      "metadata": {
        "id": "-N6Fxt6NNyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p code"
      ],
      "metadata": {
        "id": "8dqa_xicNzrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile code/preprocessing.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "COLUMN_ORDER = [\n",
        "    \"reordered\",  # Label (first column)\n",
        "    \"user_id\",\n",
        "    \"product_id\",\n",
        "    \"aisle_id\",\n",
        "    \"department_id_encoded\",\n",
        "    \"department_category\"\n",
        "]\n",
        "\n",
        "DTYPES = {\n",
        "    \"reordered\": np.float64,\n",
        "    \"user_id\": np.float64,\n",
        "    \"product_id\": np.float64,\n",
        "    \"aisle_id\": np.float64,\n",
        "    \"department_id_encoded\": np.float64,\n",
        "    \"department_category\": str\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = \"/opt/ml/processing\"\n",
        "\n",
        "    # Load data with correct column order and dtypes\n",
        "    df = pd.read_csv(\n",
        "        f\"{base_dir}/input/02_19_2025_train_data_transformed_mapped.csv\",\n",
        "        header=0,\n",
        "        usecols=COLUMN_ORDER,\n",
        "        dtype=DTYPES\n",
        "    )\n",
        "\n",
        "    # Separate label and features\n",
        "    y = df.pop(\"reordered\").values.reshape(-1, 1)  # Shape: (n_samples, 1)\n",
        "    X = df\n",
        "\n",
        "    # Define transformers\n",
        "    numeric_features = [col for col in X.columns if col != \"department_category\"]\n",
        "    numeric_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_features = [\"department_category\"]\n",
        "    categorical_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # Force dense output\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "    # Preprocess features (output is dense)\n",
        "    X_pre = preprocessor.fit_transform(X)  # Shape: (n_samples, n_features)\n",
        "\n",
        "    # Combine and shuffle\n",
        "    data = np.hstack((y, X_pre))  # Works because both are 2D\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    # Split into train, validation, test\n",
        "    train, val, test = np.split(data, [int(0.7*len(data)), int(0.85*len(data))])\n",
        "\n",
        "    # Save datasets\n",
        "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
        "    pd.DataFrame(val).to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
        "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
      ],
      "metadata": {
        "id": "yocu9HKvN3xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.sklearn.processing import SKLearnProcessor\n",
        "\n",
        "\n",
        "framework_version = \"1.2-1\"\n",
        "\n",
        "sklearn_processor = SKLearnProcessor(\n",
        "    framework_version=framework_version,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    instance_count=processing_instance_count,\n",
        "    base_job_name=\"sklearn-instacart-process\",\n",
        "    role=role,\n",
        "    sagemaker_session=pipeline_session,\n",
        ")"
      ],
      "metadata": {
        "id": "HE6KNWJZN6KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
        "from sagemaker.workflow.steps import ProcessingStep\n",
        "\n",
        "processor_args = sklearn_processor.run(\n",
        "    inputs=[\n",
        "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
        "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
        "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
        "    ],\n",
        "    code=\"code/preprocessing.py\",\n",
        ")\n",
        "\n",
        "step_process = ProcessingStep(name=\"InstaCartProcess\", step_args=processor_args)"
      ],
      "metadata": {
        "id": "5ib3KvHxN6_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Training Step to Train a Model"
      ],
      "metadata": {
        "id": "zpxkAywAN9-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.estimator import Estimator\n",
        "from sagemaker.inputs import TrainingInput\n",
        "\n",
        "model_path = f\"s3://{default_bucket}/InstaCartTrain\"\n",
        "image_uri = sagemaker.image_uris.retrieve(\n",
        "    framework=\"xgboost\",\n",
        "    region=region,\n",
        "    version=\"1.0-1\",\n",
        "    py_version=\"py3\",\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        ")\n",
        "xgb_train = Estimator(\n",
        "    image_uri=image_uri,\n",
        "    instance_type=instance_type,\n",
        "    instance_count=1,\n",
        "    output_path=model_path,\n",
        "    role=role,\n",
        "    sagemaker_session=pipeline_session,\n",
        ")\n",
        "xgb_train.set_hyperparameters(\n",
        "    objective=\"reg:squarederror\",\n",
        "    num_round=50,\n",
        "    max_depth=5,\n",
        "    eta=0.2,\n",
        "    gamma=4,\n",
        "    min_child_weight=6,\n",
        "    subsample=0.7,\n",
        ")\n",
        "\n",
        "train_args = xgb_train.fit(\n",
        "    inputs={\n",
        "        \"train\": TrainingInput(\n",
        "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
        "            content_type=\"text/csv\",\n",
        "        ),\n",
        "        \"validation\": TrainingInput(\n",
        "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
        "                \"validation\"\n",
        "            ].S3Output.S3Uri,\n",
        "            content_type=\"text/csv\",\n",
        "        ),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "J2NnlWFsOAp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.inputs import TrainingInput\n",
        "from sagemaker.workflow.steps import TrainingStep\n",
        "\n",
        "\n",
        "step_train = TrainingStep(\n",
        "    name=\"InstaCartTrain\",\n",
        "    step_args=train_args,\n",
        ")"
      ],
      "metadata": {
        "id": "6nixpopMOFyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Model Evaluation Step to Evaluate the Trained Model"
      ],
      "metadata": {
        "id": "G2bXwHAIOG64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile code/evaluation.py\n",
        "import json\n",
        "import pathlib\n",
        "import pickle\n",
        "import tarfile\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
        "    with tarfile.open(model_path) as tar:\n",
        "        tar.extractall(path=\".\")\n",
        "\n",
        "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
        "\n",
        "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
        "    df = pd.read_csv(test_path, header=None)\n",
        "\n",
        "    y_test = df.iloc[:, 0].to_numpy()\n",
        "    df.drop(df.columns[0], axis=1, inplace=True)\n",
        "\n",
        "    X_test = xgboost.DMatrix(df.values)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    std = np.std(y_test - predictions)\n",
        "    report_dict = {\n",
        "        \"regression_metrics\": {\n",
        "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    output_dir = \"/opt/ml/processing/evaluation\"\n",
        "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
        "    with open(evaluation_path, \"w\") as f:\n",
        "        f.write(json.dumps(report_dict))"
      ],
      "metadata": {
        "id": "C9mTuFB7OJFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.processing import ScriptProcessor\n",
        "\n",
        "\n",
        "script_eval = ScriptProcessor(\n",
        "    image_uri=image_uri,\n",
        "    command=[\"python3\"],\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    instance_count=1,\n",
        "    base_job_name=\"script-instacart-eval\",\n",
        "    role=role,\n",
        "    sagemaker_session=pipeline_session,\n",
        ")\n",
        "\n",
        "eval_args = script_eval.run(\n",
        "    inputs=[\n",
        "        ProcessingInput(\n",
        "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
        "            destination=\"/opt/ml/processing/model\",\n",
        "        ),\n",
        "        ProcessingInput(\n",
        "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
        "            destination=\"/opt/ml/processing/test\",\n",
        "        ),\n",
        "    ],\n",
        "    outputs=[\n",
        "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
        "    ],\n",
        "    code=\"code/evaluation.py\",\n",
        ")"
      ],
      "metadata": {
        "id": "tLDXyLfxONrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.workflow.properties import PropertyFile\n",
        "\n",
        "\n",
        "evaluation_report = PropertyFile(\n",
        "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
        ")\n",
        "step_eval = ProcessingStep(\n",
        "    name=\"InstaCartEval\",\n",
        "    step_args=eval_args,\n",
        "    property_files=[evaluation_report],\n",
        ")"
      ],
      "metadata": {
        "id": "Af3qzTAkOP2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Create Model Step to Create a Model"
      ],
      "metadata": {
        "id": "DWIobc5rOQgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.model import Model\n",
        "\n",
        "model = Model(\n",
        "    image_uri=image_uri,\n",
        "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
        "    sagemaker_session=pipeline_session,\n",
        "    role=role,\n",
        ")"
      ],
      "metadata": {
        "id": "MwkDwlDFOTJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.inputs import CreateModelInput\n",
        "from sagemaker.workflow.model_step import ModelStep\n",
        "\n",
        "step_create_model = ModelStep(\n",
        "    name=\"InstaCartCreateModel\",\n",
        "    step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
        ")"
      ],
      "metadata": {
        "id": "vOoEWNwnOVcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Transform Step to Perform Batch Transformation"
      ],
      "metadata": {
        "id": "Q4NJ2QLxOYAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.transformer import Transformer\n",
        "\n",
        "\n",
        "transformer = Transformer(\n",
        "    model_name=step_create_model.properties.ModelName,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    instance_count=1,\n",
        "    output_path=f\"s3://{default_bucket}/InstaCartTransform\",\n",
        ")"
      ],
      "metadata": {
        "id": "TzfCuXPeOXPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.inputs import TransformInput\n",
        "from sagemaker.workflow.steps import TransformStep\n",
        "\n",
        "\n",
        "step_transform = TransformStep(\n",
        "    name=\"InstaCartTransform\", transformer=transformer, inputs=TransformInput(data=batch_data)\n",
        ")"
      ],
      "metadata": {
        "id": "8iAgabQwOefq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Register Model Step to Create a Model Package"
      ],
      "metadata": {
        "id": "_Ve4Kx0MOg1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
        "\n",
        "model_metrics = ModelMetrics(\n",
        "    model_statistics=MetricsSource(\n",
        "        s3_uri=\"{}/evaluation.json\".format(\n",
        "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
        "        ),\n",
        "        content_type=\"application/json\",\n",
        "    )\n",
        ")\n",
        "\n",
        "register_args = model.register(\n",
        "    content_types=[\"text/csv\"],\n",
        "    response_types=[\"text/csv\"],\n",
        "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
        "    transform_instances=[\"ml.m5.xlarge\"],\n",
        "    model_package_group_name=model_package_group_name,\n",
        "    approval_status=model_approval_status,\n",
        "    model_metrics=model_metrics,\n",
        ")\n",
        "step_register = ModelStep(name=\"InstaCartRegisterModel\", step_args=register_args)"
      ],
      "metadata": {
        "id": "p7HAMqnnOhlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Fail Step to Terminate the Pipeline Execution and Mark it as Failed"
      ],
      "metadata": {
        "id": "SDUrLLM9Olzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.workflow.fail_step import FailStep\n",
        "from sagemaker.workflow.functions import Join\n",
        "\n",
        "step_fail = FailStep(\n",
        "    name=\"InstaCartMSEFail\",\n",
        "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE >\", mse_threshold]),\n",
        ")"
      ],
      "metadata": {
        "id": "RzVrsiDPOmgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Condition Step to Check Accuracy and Conditionally Create a Model and Run a Batch Transformation and Register a Model in the Model Registry, Or Terminate the Execution in Failed State"
      ],
      "metadata": {
        "id": "UkRS4bQxOqnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
        "from sagemaker.workflow.condition_step import ConditionStep\n",
        "from sagemaker.workflow.functions import JsonGet\n",
        "\n",
        "\n",
        "cond_lte = ConditionLessThanOrEqualTo(\n",
        "    left=JsonGet(\n",
        "        step_name=step_eval.name,\n",
        "        property_file=evaluation_report,\n",
        "        json_path=\"regression_metrics.mse.value\",\n",
        "    ),\n",
        "    right=mse_threshold,\n",
        ")\n",
        "\n",
        "step_cond = ConditionStep(\n",
        "    name=\"InstaCartMSECond\",\n",
        "    conditions=[cond_lte],\n",
        "    if_steps=[step_register, step_create_model, step_transform],\n",
        "    else_steps=[step_fail],\n",
        ")"
      ],
      "metadata": {
        "id": "qqdkrNJ8Orau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Pipeline of Parameters, Steps, and Conditions"
      ],
      "metadata": {
        "id": "YdGUViT4Ouqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.workflow.pipeline import Pipeline\n",
        "\n",
        "\n",
        "pipeline_name = f\"InstaCartPipeline\"\n",
        "pipeline = Pipeline(\n",
        "    name=pipeline_name,\n",
        "    parameters=[\n",
        "        processing_instance_count,\n",
        "        instance_type,\n",
        "        model_approval_status,\n",
        "        input_data,\n",
        "        batch_data,\n",
        "        mse_threshold,\n",
        "    ],\n",
        "    steps=[step_process, step_train, step_eval, step_cond],\n",
        ")"
      ],
      "metadata": {
        "id": "SsEMdeVvOwqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) Examining the pipeline definition"
      ],
      "metadata": {
        "id": "UeuYsx-ROzYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "definition = json.loads(pipeline.definition())\n",
        "definition"
      ],
      "metadata": {
        "id": "fAFnqB9BO1ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the pipeline to SageMaker and start execution"
      ],
      "metadata": {
        "id": "7ivPqaDoO5_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.upsert(role_arn=role)"
      ],
      "metadata": {
        "id": "D5Lg59V7O62e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution = pipeline.start()"
      ],
      "metadata": {
        "id": "oa-apcoHO-y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Operations: Examining and Waiting for Pipeline Execution"
      ],
      "metadata": {
        "id": "Xt09lFQsPBgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "execution.describe()"
      ],
      "metadata": {
        "id": "KtJf3pYXPCW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution.wait()"
      ],
      "metadata": {
        "id": "5Vqb6y3yPGQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution.list_steps()"
      ],
      "metadata": {
        "id": "KaZHxtERPIUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examining the Evaluation"
      ],
      "metadata": {
        "id": "-TXpVtFiPK8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "\n",
        "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
        "    \"{}/evaluation.json\".format(\n",
        "        step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
        "    )\n",
        ")\n",
        "pprint(json.loads(evaluation_json))"
      ],
      "metadata": {
        "id": "bhheqTUrPLr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lineage"
      ],
      "metadata": {
        "id": "MLnNk9AfPO2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
        "\n",
        "\n",
        "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
        "for execution_step in reversed(execution.list_steps()):\n",
        "    print(execution_step)\n",
        "    display(viz.show(pipeline_execution_step=execution_step))\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "id": "ycLv_nkoPQ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parametrized Executions"
      ],
      "metadata": {
        "id": "PuxcrMFOPUL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "execution = pipeline.start(\n",
        "    parameters=dict(\n",
        "        ModelApprovalStatus=\"Approved\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "d1uGY9gyPZ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution.wait()"
      ],
      "metadata": {
        "id": "pjythwSKPb13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution.list_steps()"
      ],
      "metadata": {
        "id": "6_IIa2EqPerI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution = pipeline.start(parameters=dict(MseThreshold=3.0))"
      ],
      "metadata": {
        "id": "psdh_fgpPgo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    execution.wait()\n",
        "except Exception as error:\n",
        "    print(error)"
      ],
      "metadata": {
        "id": "Nwu6hBwsPiR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution.list_steps()"
      ],
      "metadata": {
        "id": "7Vp7v3bqPj8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We thank you for taking the time to review our project."
      ],
      "metadata": {
        "id": "KFPP3vIsPmSS"
      }
    }
  ]
}
